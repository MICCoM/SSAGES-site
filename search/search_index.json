{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location": "/archive/", "text": "news archive2020 may 14 2020 replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 feb 12 2020 google summer of code julia summer of code 2019 dec 14 2019 turing s blog", "title": "Articles"},{"location": "/feed.xml", "text": "turing jl turing a robust efficient and modular library for general purpose probabilistic programming dev tue 26 may 2020 17 29 42 0000 tue 26 may 2020 17 29 42 0000 jekyll v4 0 1 replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 lt p gt the turing jl team is currently exploring possibilities in an attempt to help with the ongoing sars cov 2 crisis as preparation for this and to get our feet wet we decided to perform a replication study of the lt a href quot https www imperial ac uk mrc global infectious disease analysis covid 19 report 13 europe npi impact quot gt imperial report 13 lt a gt which attempts to estimate the real number of infections and impact of non pharmaceutical interventions on covid 19 in the report the inference was performed using the probabilistic programming language ppl stan we have explicated their model and inference in turing jl a julia based ppl we believe the results and analysis of our study are relevant for the public and for other researchers who are actively working on epidemiological models to that end our implementation and results are available lt a href quot https github com cambridge mlg covid19 quot gt here lt a gt lt p gt lt p gt in summary we replicated the imperial covid 19 model using turing jl subsequently we compared the inference results between turing and stan and our comparison indicates that results are reproducible with two different implementations in particular we performed 4 sets of simulations using the imperial covid 19 model the resulting estimates of the expected real number of cases in contrast to the lt em gt recorded lt em gt number of cases the reproduction number r t and the expected number of deaths as a function of time and non pharmaceutical interventions npis for each simulation are shown below lt p gt lt div id quot simulation 1 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full prior json quot function err fig plotly plot quot simulation 1 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation a lt strong gt hypothetical simulation from the model without data prior predictive or non pharmaceutical interventions under the prior assumptions of the imperial covid 19 model there is a very wide range of epidemic progressions with expected cases from almost 0 to 100 of the population over time the black bar corresponds to the date of the last observation note that r t has a different time range than the other plots following the original report this shows the 100 days following the country specific lt code class quot highlighter rouge quot gt epidemic start lt code gt which is defined to be 31 days prior to the first date of 10 cumulative deaths while the other plots show the last 60 days lt p gt lt div id quot simulation 2 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full posterior json quot function err fig plotly plot quot simulation 2 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation b lt strong gt future simulation with non pharmaceutical interventions kept in place posterior predictive after incorporating the observed infection data we can see a substantially more refined range of epidemic progression the reproduction rate estimate lies in the range of 3 5 5 6 before any intervention is introduced the dotted lines correspond to observations and the black bar corresponds to the date of the last observation lt p gt lt div id quot simulation 3 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full counterfactual json quot function err fig plotly plot quot simulation 3 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation c lt strong gt future simulation with non pharmaceutical interventions removed now we see the hypothetical scenarios after incorporating infection data but with non pharmaceutical interventions removed this plot looks similar to simulation a but with a more rapid progression of the pandemic since the estimated reproduction rate is bigger than the prior assumptions the dotted lines correspond to observations and the black bar corresponds to the date of the last observation lt p gt lt div id quot simulation 4 full quot class quot plotly quot gt lt div gt lt script gt plotly d3 json quot assets figures 2020 05 04 imperial report13 analysis full counterfactual2 json quot function err fig plotly plot quot simulation 4 full quot fig data fig layout responsive true lt script gt lt p gt lt strong gt simulation d lt strong gt future simulation with when lt code class quot highlighter rouge quot gt lockdown lt code gt is lifted two weeks before the last observation predictive posterior as a result there is a clear rapid rebound of the reproduction rate comparing with simulation b we do not observe an lt em gt immediate lt em gt increase in the number of expected cases and deaths upon lifting lockdown but there is a significant difference in the number of cases and deaths in the last few days in the plot simulation d results in both greater number of cases and deaths as expected this demonstrates how the effects of lifting an intervention might not become apparent in the measurable variables e g deaths until several weeks later the dotted lines correspond to observations the black bar corresponds to the date of the last observation and the red bar indicates when lt code class quot highlighter rouge quot gt lockdown lt code gt was lifted lt p gt lt p gt overall simulation a shows the prior modelling assumptions and how these prior assumptions determine the predicted number of cases etc before seeing any data simulation b predicts the trend of the number of cases etc using estimated parameters and by keeping all the non pharmaceutical interventions in place simulation c shows the estimate in the case where none of the intervention measures are ever put in place simulation d shows the estimates in the case when the lockdown was lifted two weeks prior to the last observation while keeping all the other non pharmaceutical interventions in place lt p gt lt p gt we want to emphasise that we do not provide additional analysis of the imperial model yet nor are we aiming to make any claims about the validity or the implications of the model instead we refer to imperial report 13 for more details and analysis the purpose of this post is solely to add validation to the lt em gt inference lt em gt performed in the paper by obtaining the same results using a different probabilistic programming language ppl and by exploring whether or not turing jl can be useful for researchers working on these problems lt p gt lt p gt for our next steps we re looking at collaboration with other researchers and further developments of this and similar models there are some immediate directions to explore lt p gt lt ol gt lt li gt incoporation of more sources of data e g national mobility seasonal changes and behavior changes in individuals lt li gt lt li gt how the assumptions incorporated into the priors and their parameters change resulting posterior lt li gt lt li gt the current model does not directly include recovery as a possibility and assumes that if a person has been infected once then he she will be infectious until death number of recovered cases suffers from the same issues as the number of cases it cannot be directly observed but we can also deal with it in a similar manner as is done with number of cases and incorporate this into the model for a potential improvement this will result in a plethora of different models from which we can select the most realistic one using different model comparions techniques e g leave one out cross validation loo cv lt li gt lt ol gt lt p gt such model refinement can be potentially valuable given the high impact of this pandemic and the uncertainty and debates in the potential outcomes lt p gt lt p gt lt strong gt acknowledgement lt strong gt lt em gt we would like to thank the julia community for creating such an excellent platform for scientific computing and for the continuous feedback that we have received we also thank researchers from computational and biological laboratory at cambridge university for their feedback on an early version of the post lt em gt lt footnotes gt lt p gt thu 14 may 2020 00 00 00 0000 dev posts 2020 05 04 imperial report13 analysis dev posts 2020 05 04 imperial report13 analysis google summer of code julia summer of code lt p gt last year turing participated in the google summer of code gsoc through the julia language organization it was a fun time and the project was better for it turing plans to participate in the upcoming gsoc and we wanted to outline some potential projects and expectations we have for applicants lt p gt lt p gt if you are not aware google provides funds to students around the world to develop a project of their choice over the summer students receive funds from google and spend three months on any open source project lt p gt lt p gt the turing development team has prepared a list of possible projects that we have deemed valuable to the project and easy enough that it could feasibly be created in the three month limit this list is not exlusive if you have a good idea you can write it up in your proposal though it is recommend that you reach out to any of the turing team on julia s lt a href quot https julialang slack com quot gt slack lt a gt you can get an invite lt a href quot https slackinvite julialang org quot gt here lt a gt or lt a href quot https discourse julialang org c domain probprog quot gt discourse lt a gt messages on discourse should be posted to the probabilistic programming category we ll find you lt p gt lt p gt possible project ideas lt p gt lt ul gt lt li gt lt strong gt benchmarking lt strong gt turing s performance has been sporadically benchmarked against various other probabilistic programming languages e g turing stan pymc3 tensorflow prob but a systemic approach to studying where turing excels and where it falls short would be useful a gsoc student would implement identical models in many ppls and build tools to benchmark all ppls against one another lt li gt lt li gt lt strong gt nested sampling integration lt strong gt turing focuses on modularity in inference methods and the development team would like to see more inference methods particularly the popular nested sampling method a julia package lt a href quot https github com mileslucas nestedsamplers jl quot gt nestedsamplers jl lt a gt but it is not hooked up to turing and does not currently have a stable api a gsoc student would either integrate that package or construct their own nested sampling method and build it into turing lt li gt lt li gt lt strong gt automated function memoization by model annotation lt strong gt function memoization is a way to reduce costly function evaluation by caching the output when the same inputs are given turing s gibbs sampler often ends up lt a href quot https turing ml dev docs using turing performancetips reuse computations in gibbs sampling quot gt rerunning expensive functions lt a gt multiple times and it would be a significant performance improvement to allow turing s model compiler to automatically memoize functions where appropriate a student working on this project would become intimately familiar with turing s model compiler and build in various automated improvements lt li gt lt li gt lt strong gt making distributions gpu compatible lt strong gt julia s gpu tooling is generally quite good but currently turing is not able to reliably use gpus while sampling because lt a href quot https github com juliastats distributions jl quot gt distributions jl lt a gt is not gpu compatible a student on this project would work with the turing developers and the distributions developers to allow the use of gpu parallelism where possible in turing lt li gt lt li gt lt strong gt static distributions lt strong gt small fixed size vectors and matrices are fairly common in turing models this means that sampling in turing can probably benefit from using statically sized vectors and matrices from lt a href quot https github com juliaarrays staticarrays jl quot gt staticarrays jl lt a gt instead of the dynamic normal julia arrays beside the often superior performance of small static vectors and matrices static arrays are also automatically compatible with the gpu stack in julia currently the main obstacle to using staticarrays jl is that distributions in lt a href quot https github com juliastats distributions jl quot gt distributions jl lt a gt are not compatible with staticarrays a gsoc student would adapt the multivariate and matrix variate distributions as well as the univariate distribution with vector parameters in distributions jl to make a spin off package called staticdistributions jl the student would then benchmark staticdistributions jl against distributions jl and showcase an example of using staticdistributions jl together with lt a href quot https github com juliagpu cuarrays jl quot gt cuarrays jl lt a gt and or lt a href quot https github com juliagpu cudanative jl quot gt cudanative jl lt a gt for gpu acceleration lt li gt lt li gt lt strong gt gpnet extensions lt strong gt one of turing s sattelite packages lt a href quot https github com turinglang gpnet jl quot gt gpnet lt a gt is designed to provide a comprehensive suite of gaussian process tools see lt a href quot https github com turinglang gpnet jl issues 2 quot gt this issue lt a gt for potential tasks there s a lot of interesting stuff going on with gps and this task in particular may have some creative freedom to it lt li gt lt li gt lt strong gt better chains and model diagnostics lt strong gt one package that turing and many others rely on heavily is lt a href quot https github com turinglang mcmcchains jl quot gt mcmcchains jl lt a gt a package designed to format store and analyze parameter samples generated during mcmc inference mcmcchains is currently showing its age a little and has many lt a href quot https github com turinglang mcmcchains jl issues 171 quot gt bad design choices lt a gt that need to be fixed alternatively a student could contstruct a far more lightweight chain system lt li gt lt li gt lt strong gt model comparison tools lt strong gt turing and its sattelite packages do not currently provide a comprehensive suite of model comparison tools a critical tool for the applied statistician a student who worked on this project would implement various model comparison tools like lt a href quot https mc stan org loo quot gt loo and waic lt a gt among others lt li gt lt li gt lt strong gt mle map tools lt strong gt lt a href quot https en wikipedia org wiki maximum likelihood estimation quot gt maximum likelihood estimates lt a gt mle and lt a href quot https en wikipedia org wiki maximum a posteriori estimation quot gt maximum a posteriori lt a gt map estimates can currently only be done by users through a lt a href quot https turing ml dev docs using turing advanced maximum a posteriori estimation quot gt clunky set of workarounds lt a gt a streamlined function like lt code class quot highlighter rouge quot gt mle model lt code gt or lt code class quot highlighter rouge quot gt map model lt code gt would be very useful for many of turing s users who want to see what the mle or map estimates look like and it may be valuable to allow for functionality that allows mcmc sampling to begin from the mle or map estimates students working on this project will work with optimization packages such as lt a href quot https github com julianlsolvers optim jl quot gt optim jl lt a gt to make mle and map estimation straightforward for turing models lt li gt lt li gt lt strong gt particle sampler improvements lt strong gt turing s development team has spent a lot of time and energy to make inference methods more modular but turing s particle samplers have not yet been modernized and spun off into a separate package two packages that resulted from this were lt a href quot https github com turinglang advancedhmc jl quot gt advancedhmc lt a gt for hamiltonian mcmc methods and lt a href quot https github com turinglang advancedmh jl quot gt advancedmh lt a gt for metropolis hastings style inference methods a student who worked on this project would become very familiar with turing s inference backend and with particle sampling methods this is a good project for people who love making things efficient and easily extendable lt li gt lt ul gt lt p gt other projects are welcome but we do strongly recommend discussing any potential projects with members of the turing team as they will end up mentoring gsoc students for the duration of the project lt p gt lt p gt we re looking forward to what people are interested in lt p gt wed 12 feb 2020 00 00 00 0000 dev posts 2020 02 12 jsoc dev posts 2020 02 12 jsoc turing s blog lt p gt all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like lt p gt lt ul gt lt li gt interesting things you can do with turing or interesting things we have seen others do lt li gt lt li gt development updates and major release announcements lt li gt lt li gt research updates lt li gt lt li gt explorations of turing s internals lt li gt lt li gt updates to turing s sattelite projects lt a href quot https github com turinglang advancedhmc jl quot gt advancedhmc jl lt a gt or lt a href quot https github com turinglang bijectors jl quot gt bijectors jl lt a gt lt li gt lt ul gt lt p gt stay tuned lt p gt sat 14 dec 2019 00 00 00 0000 dev posts 2019 12 14 initial post dev posts 2019 12 14 initial post", "title": ""},{"location": "/", "text": "turing jl bayesian inference with probabilistic programming intuitive turing models are easy to read and write models work the way you write them general purpose turing supports models with discrete parameters and stochastic control flow specify complex models quickly and easily modular turing is modular written fully in julia and can be modified to suit your needs hello world in turing linear gaussian model turing s modelling syntax allows you to specify a model quickly and easily straightforward models can be expressed in the same way as complex hierarchical models with stochastic control flow quick start model gdemo x y begin assumptions inversegamma 2 3 normal 0 sqrt observations x normal sqrt y normal sqrt end news feed replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 may 14 2020 google summer of code julia summer of code february 12 2020 turing s blog december 14 2019 news advanced markov chain monte carlo samplers turing provides hamiltonian monte carlo sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and gibbs sampling which combines particle mcmc hmc and many other mcmc algorithms samplers interoperable with deep learning libraries turing supports julia s flux package for automatic differentiation combine turing and flux to construct probabalistic variants of traditional machine learning models bayesian neural network tutorial community join the turing community to contribute learn and get your questions answered github report bugs request features discuss issues and more go to github turing jl discuss browse and join discussions on turing go to turing jl discuss slack discuss advanced topics request access here go to slack ecosystem explore a rich ecosystem of libraries tools and more to support development advancedhmc robust modular and efficient implementation of advanced hamiltonian monte carlo algorithms go to advancedhmc mcmcchains chain types and utility functions for mcmc simulations go to mcmcchains bijectors automatic transformations for constrained random variables go to bijectors", "title": "Turing.jl - Turing.jl"},{"location": "/news/", "text": "newssubscribe with rss to keep up with the latest news about turing replication study estimating number of infections and impact of npis on covid 19 in european countries imperial report 13 may 14 2020 the turing jl team is currently exploring possibilities in an attempt to help with the ongoing sars cov 2 crisis as preparation for this and to get our feet wet we decided to perform a replication study of the imperial report 13 which attempts to estimate the real number of infections and impact of non pharmaceutical interventions on covid 19 in the report the inference was performed using the probabilistic programming language ppl stan we have explicated their model and inference in turing jl a julia based ppl we believe the results and analysis of our study are relevant for the public and for other researchers who are actively working on epidemiological models to that end our implementation and results are available here read more google summer of code julia summer of code february 12 2020 last year turing participated in the google summer of code gsoc through the julia language organization it was a fun time and the project was better for it turing plans to participate in the upcoming gsoc and we wanted to outline some potential projects and expectations we have for applicants read more turing s blog december 14 2019 all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like read more want to see more see the news archive", "title": "News"},{"location": "/search/search_index.json", "text": "config lang en prebuild index false separator s docs for page in site pages unless page excluded in search if added endif assign added false location page url text page content strip html strip newlines slugify ascii replace title page title assign added true endunless endfor for post in site posts unless page excluded in search if added endif assign added false location post url text post content strip html strip newlines slugify ascii replace title post title assign added true endunless endfor for doc in site docs unless doc excluded in search if added endif assign added false location doc url text doc content strip html strip newlines slugify ascii replace title doc title assign added true endunless endfor", "title": ""},{"location": "/sitemap.xml", "text": "now date y m d daily for section in site data toc site baseurl section url now date y m d daily endfor", "title": ""},{"location": "/robots.txt", "text": "sitemap sitemap xml absolute url", "title": ""},{"location": "/feed.xml", "text": "if page xsl endif jekyll site time date to xmlschema page url absolute url xml escape assign title site title default site name if page collection posts assign collection page collection capitalize assign title title append append collection endif if page category assign category page category capitalize assign title title append append category endif if title title smartify xml escape endif if site description site description xml escape endif if site author site author name default site author xml escape if site author email site author email xml escape endif if site author uri site author uri xml escape endif endif assign posts site page collection where exp post post draft true sort date reverse if page category assign posts posts where category page category endif for post in posts limit 10 post title smartify strip html normalize whitespace xml escape post date date to xmlschema post last modified at default post date date to xmlschema post id absolute url xml escape assign excerpt only post feed excerpt only default site feed excerpt only unless excerpt only post content strip xml escape endunless assign post author post author default post authors 0 default site author assign post author site data authors post author default post author assign post author email post author email default nil assign post author uri post author uri default nil assign post author name post author name default post author post author name default xml escape if post author email post author email xml escape endif if post author uri post author uri xml escape endif if post category endif for tag in post tags endfor if post excerpt and post excerpt empty post excerpt strip html normalize whitespace xml escape endif assign post image post image path default post image if post image unless post image contains assign post image post image absolute url endunless endif endfor", "title": ""},{"location": "/posts/2020-05-04-Imperial-Report13-analysis", "text": "the turing jl team is currently exploring possibilities in an attempt to help with the ongoing sars cov 2 crisis as preparation for this and to get our feet wet we decided to perform a replication study of the imperial report 13 which attempts to estimate the real number of infections and impact of non pharmaceutical interventions on covid 19 in the report the inference was performed using the probabilistic programming language ppl stan we have explicated their model and inference in turing jl a julia based ppl we believe the results and analysis of our study are relevant for the public and for other researchers who are actively working on epidemiological models to that end our implementation and results are available here in summary we replicated the imperial covid 19 model using turing jl subsequently we compared the inference results between turing and stan and our comparison indicates that results are reproducible with two different implementations in particular we performed 4 sets of simulations using the imperial covid 19 model the resulting estimates of the expected real number of cases in contrast to the recorded number of cases the reproduction number r t and the expected number of deaths as a function of time and non pharmaceutical interventions npis for each simulation are shown below simulation a hypothetical simulation from the model without data prior predictive or non pharmaceutical interventions under the prior assumptions of the imperial covid 19 model there is a very wide range of epidemic progressions with expected cases from almost 0 to 100 of the population over time the black bar corresponds to the date of the last observation note that r t has a different time range than the other plots following the original report this shows the 100 days following the country specific epidemic start which is defined to be 31 days prior to the first date of 10 cumulative deaths while the other plots show the last 60 days simulation b future simulation with non pharmaceutical interventions kept in place posterior predictive after incorporating the observed infection data we can see a substantially more refined range of epidemic progression the reproduction rate estimate lies in the range of 3 5 5 6 before any intervention is introduced the dotted lines correspond to observations and the black bar corresponds to the date of the last observation simulation c future simulation with non pharmaceutical interventions removed now we see the hypothetical scenarios after incorporating infection data but with non pharmaceutical interventions removed this plot looks similar to simulation a but with a more rapid progression of the pandemic since the estimated reproduction rate is bigger than the prior assumptions the dotted lines correspond to observations and the black bar corresponds to the date of the last observation simulation d future simulation with when lockdown is lifted two weeks before the last observation predictive posterior as a result there is a clear rapid rebound of the reproduction rate comparing with simulation b we do not observe an immediate increase in the number of expected cases and deaths upon lifting lockdown but there is a significant difference in the number of cases and deaths in the last few days in the plot simulation d results in both greater number of cases and deaths as expected this demonstrates how the effects of lifting an intervention might not become apparent in the measurable variables e g deaths until several weeks later the dotted lines correspond to observations the black bar corresponds to the date of the last observation and the red bar indicates when lockdown was lifted overall simulation a shows the prior modelling assumptions and how these prior assumptions determine the predicted number of cases etc before seeing any data simulation b predicts the trend of the number of cases etc using estimated parameters and by keeping all the non pharmaceutical interventions in place simulation c shows the estimate in the case where none of the intervention measures are ever put in place simulation d shows the estimates in the case when the lockdown was lifted two weeks prior to the last observation while keeping all the other non pharmaceutical interventions in place we want to emphasise that we do not provide additional analysis of the imperial model yet nor are we aiming to make any claims about the validity or the implications of the model instead we refer to imperial report 13 for more details and analysis the purpose of this post is solely to add validation to the inference performed in the paper by obtaining the same results using a different probabilistic programming language ppl and by exploring whether or not turing jl can be useful for researchers working on these problems for our next steps we re looking at collaboration with other researchers and further developments of this and similar models there are some immediate directions to explore incoporation of more sources of data e g national mobility seasonal changes and behavior changes in individuals how the assumptions incorporated into the priors and their parameters change resulting posterior the current model does not directly include recovery as a possibility and assumes that if a person has been infected once then he she will be infectious until death number of recovered cases suffers from the same issues as the number of cases it cannot be directly observed but we can also deal with it in a similar manner as is done with number of cases and incorporate this into the model for a potential improvement this will result in a plethora of different models from which we can select the most realistic one using different model comparions techniques e g leave one out cross validation loo cv such model refinement can be potentially valuable given the high impact of this pandemic and the uncertainty and debates in the potential outcomes acknowledgement we would like to thank the julia community for creating such an excellent platform for scientific computing and for the continuous feedback that we have received we also thank researchers from computational and biological laboratory at cambridge university for their feedback on an early version of the post", "title": "Replication study: Estimating number of infections and impact of NPIs on COVID-19 in European countries (Imperial Report 13)"},{"location": "/posts/2020-02-12-jsoc", "text": "last year turing participated in the google summer of code gsoc through the julia language organization it was a fun time and the project was better for it turing plans to participate in the upcoming gsoc and we wanted to outline some potential projects and expectations we have for applicants if you are not aware google provides funds to students around the world to develop a project of their choice over the summer students receive funds from google and spend three months on any open source project the turing development team has prepared a list of possible projects that we have deemed valuable to the project and easy enough that it could feasibly be created in the three month limit this list is not exlusive if you have a good idea you can write it up in your proposal though it is recommend that you reach out to any of the turing team on julia s slack you can get an invite here or discourse messages on discourse should be posted to the probabilistic programming category we ll find you possible project ideas benchmarking turing s performance has been sporadically benchmarked against various other probabilistic programming languages e g turing stan pymc3 tensorflow prob but a systemic approach to studying where turing excels and where it falls short would be useful a gsoc student would implement identical models in many ppls and build tools to benchmark all ppls against one another nested sampling integration turing focuses on modularity in inference methods and the development team would like to see more inference methods particularly the popular nested sampling method a julia package nestedsamplers jl but it is not hooked up to turing and does not currently have a stable api a gsoc student would either integrate that package or construct their own nested sampling method and build it into turing automated function memoization by model annotation function memoization is a way to reduce costly function evaluation by caching the output when the same inputs are given turing s gibbs sampler often ends up rerunning expensive functions multiple times and it would be a significant performance improvement to allow turing s model compiler to automatically memoize functions where appropriate a student working on this project would become intimately familiar with turing s model compiler and build in various automated improvements making distributions gpu compatible julia s gpu tooling is generally quite good but currently turing is not able to reliably use gpus while sampling because distributions jl is not gpu compatible a student on this project would work with the turing developers and the distributions developers to allow the use of gpu parallelism where possible in turing static distributions small fixed size vectors and matrices are fairly common in turing models this means that sampling in turing can probably benefit from using statically sized vectors and matrices from staticarrays jl instead of the dynamic normal julia arrays beside the often superior performance of small static vectors and matrices static arrays are also automatically compatible with the gpu stack in julia currently the main obstacle to using staticarrays jl is that distributions in distributions jl are not compatible with staticarrays a gsoc student would adapt the multivariate and matrix variate distributions as well as the univariate distribution with vector parameters in distributions jl to make a spin off package called staticdistributions jl the student would then benchmark staticdistributions jl against distributions jl and showcase an example of using staticdistributions jl together with cuarrays jl and or cudanative jl for gpu acceleration gpnet extensions one of turing s sattelite packages gpnet is designed to provide a comprehensive suite of gaussian process tools see this issue for potential tasks there s a lot of interesting stuff going on with gps and this task in particular may have some creative freedom to it better chains and model diagnostics one package that turing and many others rely on heavily is mcmcchains jl a package designed to format store and analyze parameter samples generated during mcmc inference mcmcchains is currently showing its age a little and has many bad design choices that need to be fixed alternatively a student could contstruct a far more lightweight chain system model comparison tools turing and its sattelite packages do not currently provide a comprehensive suite of model comparison tools a critical tool for the applied statistician a student who worked on this project would implement various model comparison tools like loo and waic among others mle map tools maximum likelihood estimates mle and maximum a posteriori map estimates can currently only be done by users through a clunky set of workarounds a streamlined function like mle model or map model would be very useful for many of turing s users who want to see what the mle or map estimates look like and it may be valuable to allow for functionality that allows mcmc sampling to begin from the mle or map estimates students working on this project will work with optimization packages such as optim jl to make mle and map estimation straightforward for turing models particle sampler improvements turing s development team has spent a lot of time and energy to make inference methods more modular but turing s particle samplers have not yet been modernized and spun off into a separate package two packages that resulted from this were advancedhmc for hamiltonian mcmc methods and advancedmh for metropolis hastings style inference methods a student who worked on this project would become very familiar with turing s inference backend and with particle sampling methods this is a good project for people who love making things efficient and easily extendable other projects are welcome but we do strongly recommend discussing any potential projects with members of the turing team as they will end up mentoring gsoc students for the duration of the project we re looking forward to what people are interested in", "title": "Google Summer of Code/Julia Summer of Code"},{"location": "/posts/2019-12-14-initial-post", "text": "all good open source projects should have a blog and turing is one such project later on members of the turing team may be populating this feed with posts on topics like interesting things you can do with turing or interesting things we have seen others do development updates and major release announcements research updates explorations of turing s internals updates to turing s sattelite projects advancedhmc jl or bijectors jl stay tuned", "title": "Turing's Blog"},{"location": "/docs/contributing/guide", "text": "contributingturing is an open source project if you feel that you have some relevant skills and are interested in contributing then please do get in touch you can contribute by opening issues on github or implementing things yourself and making a pull request we would also appreciate example models written using turing turing has a style guide it is not strictly necessary to review it before making a pull request but you may be asked to change portions of your code to conform with the style guide before it is merged how to contributegetting started fork this repository clone your fork on your local machine git clone https github com your username turing jl add a remote corresponding to this repository git remote add upstream https github com turinglang turing jl what can i do look at the issues page to find an outstanding issue for instance you could implement new features fix bugs or write example models git workflowfor more information on how the git workflow typically functions please see the github s introduction or julia s contribution guide", "title": "Contributing"},{"location": "/docs/contributing/style-guide", "text": "style guidethis style guide is adapted from invenia s style guide we would like to thank them for allowing us to access and use it please don t let not having read it stop you from contributing to turing no one will be annoyed if you open a pr whose style doesn t follow these conventions we will just help you correct it before it gets merged these conventions were originally written at invenia taking inspiration from a variety of sources including python s pep8 julia s notes for contributors and julia s style guide what follows is a mixture of a verbatim copy of invenia s original guide and some of our own modifications a word on consistencywhen adhering to this style it s important to realize that these are guidelines and not rules this is stated best in the pep8 a style guide is about consistency consistency with this style guide is important consistency within a project is more important consistency within one module or function is most important but most importantly know when to be inconsistent sometimes the style guide just doesn t apply when in doubt use your best judgment look at other examples and decide what looks best and don t hesitate to ask synopsisattempt to follow both the julia contribution guidelines the julia style guide and this guide when convention guidelines conflict this guide takes precedence known conflicts will be noted in this guide use 4 spaces per indentation level no tabs try to adhere to a 92 character line length limit use upper camel case convention for modules and types use lower case with underscores for method names note julia code likes to use lower case without underscores comments are good try to explain the intentions of the code use whitespace to make the code more readable no whitespace at the end of a line trailing whitespace avoid padding brackets with spaces ex int64 value preferred over int64 value editor configurationsublime text settingsif you are a user of sublime text we recommend that you have the following options in your julia syntax specific settings to modify these settings first open any julia file jl in sublime text then navigate to preferences gt settings more gt syntax specific user translate tabs to spaces true tab size 4 trim trailing white space on save true ensure newline at eof on save true rulers 92 vim settingsif you are a user of vim we recommend that you add the following options to your vimrc file set tabstop 4 sets tabstops to a width of four columns set softtabstop 4 determines the behaviour of tab and backspace keys with expandtab set shiftwidth 4 determines the results of gt gt lt lt and au filetype julia setlocal expandtab replaces tabs with spaces au filetype julia setlocal colorcolumn 93 highlights column 93 to help maintain the 92 character line limit by default vim seems to guess that jl files are written in lisp to ensure that vim recognizes julia files you can manually have it check for the jl extension but a better solution is to install julia vim which also includes proper syntax highlighting and a few cool other features atom settingsatom defaults preferred line length to 80 characters we want that at 92 for julia to change it go to atom gt preferences gt packages search for the language julia package and open the settings for it find preferred line length under julia grammar and change it to 92 code formattingfunction namingnames of functions should describe an action or property irrespective of the type of the argument the argument s type provides this information instead for example buyfood food should be buy food food names of functions should usually be limited to one or two lowercase words ideally write buyfood not buy food but if you are writing a function whose name is hard to read without underscores then please do use them method definitionsonly use short form function definitions when they fit on a single line yes foo x int64 abs x 3 no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data no foobar array data abstractarray t item t where t lt int64 t abs x abs item 3 for x in array data yes function foobar array data abstractarray t item t where t lt int64 return t abs x abs item 3 for x in array data endwhen using long form functions always use the return keyword yes function fnc x result zero x result fna x return resultend no function fnc x result zero x result fna x end yes function foo x y return new x y end no function foo x y new x y endfunctions definitions with parameter lines which exceed 92 characters should separate each parameter by a newline and indent by one level yes function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend ok function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeend no function foobar df dataframe id symbol variable symbol value abstractstring prefix abstractstring codeendkeyword argumentswhen calling a function always separate your keyword arguments from your positional arguments with a semicolon this avoids mistakes in ambiguous cases such as splatting a dict yes xy foo x y 3 no xy foo x y 3 whitespaceavoid extraneous whitespace in the following situations immediately inside parentheses square brackets or braces yes spam ham 1 eggs no spam ham 1 eggs immediately before a comma or semicolon yes if x 4 show x y x y y x endno if x 4 show x y x y y x end when using ranges unless additional operators are used yes ham 1 9 ham 1 3 9 ham 1 3 end no ham 1 9 ham 1 3 9 yes ham lower upper ham lower step upper yes ham lower offset upper offset yes ham lower offset upper offset no ham lower offset upper offset more than one space around an assignment or other operator to align it with another yes x 1y 2long variable 3 no x 1y 2long variable 3 when using parametric types yes f a abstractarray t n where t lt real n g a abstractarray lt real n where n no f a abstractarray t n where t lt real n g a abstractarray lt real n where n always surround these binary operators with a single space on either side assignment updating operators etc numeric comparisons operators lt gt etc note that this guideline does not apply when performing assignment in method definitions yes i i 1no i i 1yes submitted 1no submitted 1yes x 2 lt yno x 2 lt y assignments using expanded array tuple or function notation should have the first open bracket on the same line assignment operator and the closing bracket should match the indentation level of the assignment alternatively you can perform assignments on a single line when they are short yes arr 1 2 3 arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 arr 1 2 3 arr 1 2 3 nested array or tuples that are in expanded notation should have the opening and closing brackets at the same indentation level yes x 1 2 3 hello world a b c no y 1 2 3 hello world z 1 2 3 hello world always include the trailing comma when working with expanded arrays tuples or functions notation this allows future edits to easily move elements around or add additional elements the trailing comma should be excluded when the notation is only on a single line yes arr 1 2 3 result function arg1 arg2 arr 1 2 3 no arr 1 2 3 result function arg1 arg2 arr 1 2 3 triple quotes use the indentation of the lowest indented line excluding the opening triple quote this means the closing triple quote should be aligned to least indented line in the string triple backticks should also follow this style even though the indentation does not matter for them yes str hello world str hello world cmd program flag value parameter no str hello world commentscomments should be used to state the intended behaviour of code this is especially important when the code is doing something clever that may not be obvious upon first inspection avoid writing comments that state exactly what the code obviously does yes x x 1 compensate for border no x x 1 increment xcomments that contradict the code are much worse than no comments always make a priority of keeping the comments up to date with code changes comments should be complete sentences if a comment is a phrase or sentence its first word should be capitalized unless it is an identifier that begins with a lower case letter never alter the case of identifiers if a comment is short the period at the end can be omitted block comments generally consist of one or more paragraphs built out of complete sentences and each sentence should end in a period comments should be separated by at least two spaces from the expression and have a single space after the when referencing julia in documentation note that julia refers to the programming language while julia typically in backticks e g julia refers to the executable a commmentcode another commentmore codetododocumentationit is recommended that most modules types and functions should have docstrings that being said only exported functions are required to be documented avoid documenting methods like as the built in docstring for the function already covers the details well try to document a function and not individual methods where possible as typically all methods will have similar docstrings if you are adding a method to a function which was defined in base or another package only add a docstring if the behaviour of your function deviates from the existing docstring docstrings are written in markdown and should be concise docstring lines should be wrapped at 92 characters bar x y compute the bar index between x and y if y is missing compute the bar index betweenall pairs of columns of x function bar x y when types or methods have lots of parameters it may not be feasible to write a concise docstring in these cases it is recommended you use the templates below note if a section doesn t apply or is overly verbose for example throws if your function doesn t throw an exception it can be excluded it is recommended that you have a blank line between the headings and the content when the content is of sufficient length try to be consistent within a docstring whether you use this additional whitespace note that the additional space is only for reading raw markdown and does not effect the rendered version type template should be skipped if is redundant with the constructor s docstring myarray t n my super awesome array wrapper fields data abstractarray t n stores the array being wrapped metadata dict stores metadata about the array struct myarray t n lt abstractarray t n data abstractarray t n metadata dictendfunction template only required for exported functions mysearch array myarray t val t verbose true where t gt intsearches the array for the val for some reason we don t want to use julia sbuiltin search arguments array myarray t the array to search val t the value to search for keywords verbose bool true print out progress details returns int the index where val is located in the array throws notfounderror i guess we could throw an error if val isn t found function mysearch array abstractarray t val t where t endif your method contains lots of arguments or keywords you may want to exclude them from the method signature on the first line and instead use args and or kwargs manager args kwargs gt managera cluster manager which spawns workers arguments min workers integer the minimum number of workers to spawn or an exception is thrown max workers integer the requested number of worker to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endfeel free to document multiple methods for a function within the same docstring be careful to only do this for functions you have defined manager max workers kwargs manager min workers max workers kwargs manager min workers max workers kwargs a cluster manager which spawns workers arguments min workers int the minimum number of workers to spawn or an exception is thrown max workers int the number of requested workers to spawn keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance name abstractstring queue abstractstring function manager endif the documentation for bullet point exceeds 92 characters the line should be wrapped and slightly indented avoid aligning the text to the keywords definition abstractstring name of the job definition to use defaults to the definition used within the current instance for additional details on documenting in julia see the official documentation test formattingtestsetsjulia provides test sets which allows developers to group tests into logical groupings test sets can be nested and ideally packages should only have a single root test set it is recommended that the runtests jl file contains the root test set which contains the remainder of the tests testset pkgextreme begin include arithmetic jl include utils jl endthe file structure of the test folder should mirror that of the src folder every file in src should have a complementary file in the test folder containing tests relevant to that file s contents comparisonsmost tests are written in the form test x y since the function doesn t take types into account tests like the following are valid test 1 0 1 avoid adding visual noise into test comparisons yes test value 0 no test value 0 0in cases where you are checking the numerical validity of a model s parameter estimates please use the check numerical function found in test test utils numerical tests jl this function will evaluate a model s parameter estimates using tolerance levels atol and rtol testing will only be performed if you are running the test suite locally or if travis is executing the numerical testing stage here is an example of usage check that m and s are plus or minus one from 1 5 and 2 2 respectively check numerical chain m s 1 5 2 2 atol 1 0 checks the estimates for a default gdemo model using values 1 5 and 2 0 check gdemo chain atol 0 1 checks the estimates for a default mog model check mogtest default chain atol 0 1", "title": "Style Guide"},{"location": "/docs/for-developers/compiler", "text": "in this section i will describe the current design of turing s model compiler which enables turing to perform various types of bayesian inference without changing the model definition what we call compiler is essentially just a macro that transforms the user s code to something that julia s dispatch can operate on and that julia s compiler can successfully do type inference on for efficient machine code generation overviewthe following terminology will be used in this section d observed data variables conditioned upon in the posterior p parameter variables distributed according to the prior distributions these will also be referred to as random variables model a fully defined probabilistic model with input data and modelgen a model generator function that can be used to instantiate a model instance by inputing data d turing s model macro defines a modelgen that can be used to instantiate a model by passing in the observed data d model macro and modelgenthe following are the main jobs of the model macro parse and lines e g y normal c x 1 0 figure out if a variable belongs to the data d and or to the parameters p enable the handling of missing data variables in d when defining a model and treating them as parameter variables in p instead enable the tracking of random variables using the data structures varname and varinfo change lines with a variable in p on the lhs to a call to an assume dot assume block change lines with a variable in d on the lhs to a call to an observe dot observe block enable type stable automatic differentiation of the model using type parameterslet s take the following model as an example model gauss x missing y 1 0 type tv vector float64 where tv lt abstractvector begin if x missing x tv undef 3 end p tv undef 2 p 1 inversegamma 2 3 p 2 normal 0 1 0 x 1 2 normal p 2 sqrt p 1 x 3 normal y normal p 2 sqrt p 1 endthe above call of the model macro defines an instance of modelgen called gauss a model model can be defined using gauss rand 3 1 0 or gauss x rand 3 y 1 0 while constructing the model if an argument is not passed in it gets assigned to its default value if there is no default value given an error is thrown if an argument has a default value missing when not passed in it is treated as a random variable for variables which require an intialization because we need to loop or broadcast over its elements such as x above the following needs to be done if x missing x endif x is sampled as a whole from a multivariate distribution e g x mvnormal there is no need to initialize it in an if block modelgen is defined as struct modelgen targs f tdefaults lt function f f defaults tdefaultsendmodelgen targs args where targs modelgen targs typeof args args m modelgen args kwargs m f args kwargs targs is the tuple of the symbols of the model s arguments x y tv defaults is the namedtuple of default values x missing y 1 0 tv vector float64 the model macro is defined as macro model input expr build model info input expr gt replace tilde gt replace vi gt replace logpdf gt replace sampler gt build outputendbuild model infothe first stop that the model definition takes is build model info this function extracts some information from the model definition such as name the model name main body the model body excluding the header and end arg syms the argument symbols e g x y tv above args a modified version of the arguments changing type tv vector float64 and where tv lt abstractvector to tv type lt abstractvector vector float64 this is x missing y 1 0 tv type lt abstractvector vector float64 in the example above args nt an expression constructing a namedtuple of the input arguments e g x x y y tv tv in the example above defaults nt an expression constructing a namedtuple of the default values of the input arguments if any e g x missing y 1 tv vector float64 in the example above and returns it as a dictionary called model info replace tilde after some model information have been extracted replace tilde replaces the l r lines in the model with the output of core tilde l r model info where l and r are either expressions or symbols l can also be a constant literal the replace tilde function also replaces expressions of the form l r with the output of dot tilde l r model info in the above example p 1 inversegamma 2 3 is replaced with temp right inversegamma 2 3 turing core assert dist temp right msg preprocessed turing core preprocess val x y t turing getmissing model p 1 if preprocessed isa tuple vn inds preprocessed out turing inference tilde ctx sampler temp right vn inds vi p 1 out 1 acclogp vi out 2 else lp turing inference tilde ctx sampler temp right preprocessed vi acclogp vi lp endwhere ctx abstractcontext sampler abstractsampler and vi varinfo will be discussed later assert dist will check that the rhs of is a distribution otherwise an error is thrown the preprocess macro here checks if the symbol on the lhs of p in this case is in the arguments to the model x y t or not if it isn t then p 1 will be treated as a random variable if it is in the arguments but was among the arguments with a value of missing obtained using getmissing model then p 1 is also treated as a random variable if neither of the above is true but the value of p 1 is missing then p 1 will still be treated as a random variable otherwise p 1 is treated as an observation if preprocess treats p 1 as a random variable it will return a 2 tuple of 1 a variable identifier vn varname turing varname p 1 and 2 a tuple of tuples of the indices used in vn 1 in this example otherwise preprocess returns the value of p 1 turing varname and varname wil be explained later the above checks by preprocess were carefully written to make sure that the julia compiler can compile them away so no checks happen at runtime and only the correct branch is run straight away when the output of preprocess is a tuple i e p 1 is a random variable the turing inference tilde function will dispatch to a different method than when the output is of another type i e p 1 is an observation in the former case turing inference tilde returns 2 outputs the value of the random variable and the log probability while in the latter case only the log probability is returned the log probabilities then get accumulated and if p 1 is a random variable the first returned output by turing inference tilde gets assigned to it note that core tilde is different from inference tilde core tilde returns the expression block that will be run instead of the line a part of this expression block is a call to inference tilde as shown above core tilde is defined in the compiler jl file while inference tilde is defined in the inference jl file the dot tilde function does something similar for expressions of the form l r and l r in julia 1 1 and above let s take x 1 2 normal p 2 sqrt p 1 as an example this expressions replaced with temp right normal p 2 sqrt p 1 turing core assert dist temp right msg preprocessed turing core preprocess val x y t turing getmissing model x 1 2 if preprocessed isa tuple vn inds preprocessed temp left x 1 2 out turing inference dot tilde ctx sampler temp right temp left vn inds vi left out 1 acclogp vi out 2 else temp left preprocessed x 1 2 lp turing inference dot tilde ctx sampler temp right temp left vi acclogp vi lp endthe main difference in the expanded code between l r and l r is that the former doesn t assume l to be defined it can be a new julia variable in the scope while the latter assumes l already exists l is also always input to the dot tilde function but not the tilde function replace vi replace logpdf and replace sampler using varinfo inside the model body will give the user access to the vi varinfo object used inside the model the function replace vi therefore finds and replaces every use of varinfo with the handle to the varinfo instance used inside the model the logpdf macro will return vi logp which is the accumumlated log probability that the model is computing what this means can change depending on the context ctx used when running the model finally replace sampler will replace sampler with the sampler input to the model turing modelevery model model can be called as a function with arguments vi varinfo spl abstractsampler and ctx abstractcontext vi is a data structure that stores information about random variables in p spl includes the choice of the mcmc algorithm e g metropolis hastings importance sampling or hamiltonian monte carlo hmc ctx is used to modify the behaviour of the logp accumulator accumulating different variants of it for example if ctx isa likelihoodcontext only the log likelihood will be accumulated in vi logp by default ctx isa defaultcontext which accumulates the log joint probability of p and d the inference tilde and inference dot tilde functions will do something different for different subtypes of abstractsampler to facilitate the sampling process the model struct is defined as follows struct model f targs lt namedtuple tmodelgen tmissings lt val f f args targs modelgen tmodelgen missings tmissingsendmodel f args namedtuple modelgen model f args modelgen getmissing args model model vi model vi samplefromprior model model vi spl model vi spl defaultcontext model model args kwargs model f args model kwargs model f is an internal function that is called when model is called where model model when model is called model itself is passed as an argument to model f because we need to access model args among other things inside f model args is a namedtuple of all the arguments that were passed to the model generating function when constructing an instance of model modelgen is the instance of modelgen that was used to construct model missings is an instance of val e g val a b getmissings returns a val instance of all the symbols in args with a value missing this is the default definition of missings all variables in missings are treated as random variables rather than observations in some non traditional use cases missings is defined differently e g when computing the log joint probability of the random variables and only some observations simultaneously possibly conditioned on the remaining observations an example using the model above is logprob x rand 3 p rand 2 model gauss y nothing to evaluate this the model argument x on the lhs of is treated as a random variable leading to a call to the assume or dot assume function in place of the or expressions respectively the model is then run in the priorcontext which ignores the observe and dot observe functions and only runs the assume and dot assume ones this returns the correct log probability the reason why a model input argument such as x cannot be initialized to missing when on the lhs of is somewhat subtle in the model body before calling sometimes there would be a call to length x iterating over the elements of x in a loop calling on each element of x if x is initialized to missing this will error because length missing is not defined moreover it is not intuitive to require the user to handle the x missing case because the user never assigned x to be missing in the first place missing is merely an implementation detail in this case that the users need not concern themselves with therefore in this case it makes sense to de couple the missings field from the values of the arguments build outputnow that we have all the information we need in the model macro we can start building the model generator function the model generator function gauss will be defined as function outer function x missing y 1 0 tv type lt abstractvector vector float64 return outer function x y tv endfunction outer function x missing y 1 0 tv type lt abstractvector vector float64 function inner function vi turing varinfo sampler turing abstractsampler ctx abstractcontext model end return turing model inner function x x y y tv tv turing core modelgen x y tv outer function x missing y 1 0 tv vector float64 endgauss turing core modelgen x y tv outer function x missing y 1 0 tv vector float64 the above 2 methods enable constructing the model using positional or keyword arguments the second argument to the turing model constructor is the expression called args nt stored in model info the second argument to the modelgen constructor inside outer function and outside is the expression called defaults nt stored in model info the body of the inner function is explained below inner functionthe main method of inner function does some pre processing defining all the input variables from the model definition x y and tv in the example above then the rest of the model body is run as normal julia code with the l r and l r lines replaced with the calls to inference tilde and inference dot tilde respectively as shown earlier function inner function vi turing varinfo sampler turing abstractsampler ctx abstractcontext model temp x model args x xt typeof temp x if temp x isa turing core floatorarraytype x turing core get matching type sampler vi temp x elseif turing core hasmissing xt x turing core get matching type sampler vi xt temp x else x temp x end the code above is repeated for the other 2 variables y and tv reset the logp accumulator resetlogp vi main model bodyendas one can see above x y and tv are defined in the method body using an if block followed by the rest of the code the first branch of this if block is run if the variable is a number or array type such as tv vector float64 one of the purposes of get matching type is to check if sampler requires automatic differentiation and to modify tv accordingly for example when using forwarddiff for automatic differentiation tv will be defined as some concrete subtype of vector lt forwarddiff dual this same function is also used to replace array with libtask tarray types when a particle sampler is used the second branch of the if block is to handle partially missing data converting the type of the input vector to another type befitting of the sampler used whether it is for automatic differentiation or for particle samplers finally the third branch is the one that will be run for x and y above simply assigning these names to model args x and model args y respectively the main model body is then the same model body passed in by the user after replacing l r l r varinfo and logpdf as explained eariler varnamein order to track random variables in the sampling process turing uses the struct varname sym which acts as a random variable identifier generated at runtime the varname of a random variable is generated from the expression on the lhs of a statement when the symbol on the lhs is in p every vn varname sym has a symbol sym which is the symbol of the julia variable in the model that the random variable belongs to for example x 1 normal will generate an instance of varname x assuming x is in p every vn varname also has a field indexing which stores the indices requires to access the random variable from the julia variable indicated by sym for example x 1 normal will generate a vn varname x with vn indexing 1 varname also supports hierarchical arrays and range indexing some more examples x 1 normal will generate a varname x with indexing 1 x 1 mvnormal zeros 2 will generate a varname x with indexing colon 1 x 1 2 normal will generate a varname x with indexing colon 1 2 varinfooverviewvarinfo is the data structure in turing that facilitates tracking random variables and certain metadata about them that are required for sampling for instance the distribution of every random variable is stored in varinfo because we need to know the support of every random variable when sampling using hmc for example random variables whose distributions have a constrained support are transformed using a bijector from bijectors jl so that the sampling happens in the unconstrained space different samplers require different metadata about the random variables the definition of varinfo in turing is struct varinfo tmeta tlogp lt abstractvarinfo metadata tmeta logp base refvalue tlogp num produce base refvalue int endbased on the type of metadata the varinfo is either aliased untypedvarinfo or typedvarinfo metadata can be either a subtype of the union type metadata or a namedtuple of multiple such subtypes let vi be an instance of varinfo if vi isa varinfo lt metadata then it is called an untypedvarinfo if vi isa varinfo lt namedtuple then vi metadata would be a namedtuple mapping each symbol in p to an instance of metadata vi would then be called a typedvarinfo the other fields of varinfo include logp which is used to accumulate the log probability or log probability density of the variables in p and d num produce keeps track of how many observations have been made in the model so far this is incremented when running a statement when the symbol on the lhs is in d metadatathe metadata struct stores some metadata about the random variables sampled this helps query certain information about a variable such as its distribution which samplers sample this variable its value and whether this value is transformed to real space or not let md be an instance of metadata md vns is the vector of all varname instances let vn be an arbitrary element of md vns md idcs is the dictionary that maps each varname instance to its index inmd vns md ranges md dists md orders and md flags md vns md idcs vn vn md dists md idcs vn is the distribution of vn md gids md idcs vn is the set of algorithms used to sample vn this is used inthe gibbs sampling process md orders md idcs vn is the number of observe statements before vn is sampled md ranges md idcs vn is the index range of vn in md vals md vals md ranges md idcs vn is the linearized vector of values of corresponding to vn md flags is a dictionary of true false flags md flags flag md idcs vn is thevalue of flag corresponding to vn note that in order to make md metadata type stable all the md vns must have the same symbol and distribution type however one can have a single julia variable e g x that is a matrix or a hierarchical array sampled in partitions e g x 1 mvnormal zeros 2 1 0 x 2 mvnormal ones 2 1 0 the symbol x can still be managed by a single md metadata without hurting the type stability since all the distributions on the rhs of are of the same type however in turing models one cannot have this restriction so we must use a type unstable metadata if we want to use one metadata instance for the whole model this is what untypedvarinfo does a type unstable metadata will still work but will have inferior performance to strike a balance between flexibility and performance when constructing the spl sampler instance the model is first run by sampling the parameters in p from their priors using an untypedvarinfo i e a type unstable metadata is used for all the variables then once all the symbols and distribution types have been identified a vi typedvarinfo is constructed where vi metadata is a namedtuple mapping each symbol in p to a specialized instance of metadata so as long as each symbol in p is sampled from only one type of distributions vi typedvarinfo will have fully concretely typed fields which brings out the peak performance of julia", "title": "Turing Compiler Design"},{"location": "/docs/for-developers/interface", "text": "the sampling interfaceturing implements a sampling interface hosted at abstractmcmc that is intended to provide a common framework for markov chain monte carlo samplers the interface presents several structures and functions that one needs to overload in order to implement an interface compatible sampler this guide will demonstrate how to implement the interface without turing interface overviewany implementation of an inference method that uses the abstractmcmc interface should implement a subset of the following types and functions a subtype of abstractsampler defined as a mutable struct containing state information or sampler parameters a function sample init which performs any necessary set up default do not perform any set up a function step which returns a transition that represents a single draw from the sampler a function transitions init which returns a container for the transitions obtained from the sampler default return a vector t of length n where t is the type of the transition obtained in the first step and n is the number of requested samples a function transitions save which saves transitions to the container default save the transition of iteration i at position i in the vector of transitions a function sample end which handles any sampler wrap up default do not perform any wrap up a function bundle samples which accepts the container of transitions and returns a collection of samples default return the vector of transitions the interface methods with exclamation points are those that are intended to allow for state mutation any mutating function is meant to allow mutation where needed you might use sample init to run some kind of sampler preparation before sampling begins this could mutate a sampler s state step might mutate a sampler flag after each sample sample end contains any wrap up you might need to do if you were sampling in a transformed space this might be where you convert everything back to a constrained space why do you have an interface the motivation for the interface is to allow julia s fantastic probabilistic programming language community to have a set of standards and common implementations so we can all thrive together markov chain monte carlo methods tend to have a very similar framework to one another and so a common interface should help more great inference methods built in single purpose packages to experience more use among the community implementing metropolis hastings without turingmetropolis hastings is often the first sampling method that people are exposed to it is a very straightforward algorithm and is accordingly the easiest to implement so it makes for a good example in this section you will learn how to use the types and functions listed above to implement the metropolis hastings sampler using the mcmc interface the full code for this implementation is housed in advancedmh jl importslet s begin by importing the relevant libraries we ll import abstracmcmc which contains the interface framework we ll fill out we also need distributions and random import the relevant libraries import abstractmcmcusing distributionsusing randoman interface extension like the one we re writing right now typically requires that you overload or implement several functions specifically you should import the functions you intend to overload this next code block accomplishes that from distributions we need sampleable variateform and valuesupport three abstract types that define a distribution models in the interface are assumed to be subtypes of sampleable variateform valuesupport in this section our model is going be be extremely simple so we will not end up using these except to make sure that the inference functions are dispatching correctly samplerlet s begin our sampler definition by defining a sampler called metropolishastings which is a subtype of abstractsampler correct typing is very important for proper interface implementation if you are missing a subtype your method may not be dispatched to when you call sample define a sampler type struct metropolishastings t d lt abstractmcmc abstractsampler init t proposal dend default constructors metropolishastings init real metropolishastings init normal 0 1 metropolishastings init vector lt real metropolishastings init mvnormal length init 1 above we have defined a sampler that stores the initial parameterization of the prior and a distribution object from which proposals are drawn you can have a struct that has no fields and simply use it for dispatching onto the relevant functions or you can store a large amount of state information in your sampler the general intuition for what to store in your sampler struct is that anything you may need to perform inference between samples but you don t want to store in a transition should go into the sampler struct it s the only way you can carry non sample related state information between step calls modelnext we need to have a model of some kind a model is a struct that s a subtype of abstractmodel that contains whatever information is necessary to perform inference on your problem in our case we want to know the mean and variance parameters for a standard normal distribution so we can keep our model to the log density of a normal note that we only have to do this because we are not yet integrating the sampler with turing turing has a very sophisticated modelling engine that removes the need to define custom model structs define a model type stores the log density function struct densitymodel f lt function lt abstractmcmc abstractmodel fendtransitionthe next step is to define some transition which we will return from each step call we ll keep it simple by just defining a wrapper struct that contains the parameter draws and the log density of that draw create a very basic transition type only stores the parameter draws and the log probability of the draw struct transition t l t lp lend store the new draw and its log density transition model densitymodel transition model transition can now store any type of parameter whether it s a vector of draws from multiple parameters or a single univariate draw metropolis hastingsnow it s time to get into the actual inference we ve defined all of the core pieces we need but we need to implement the step function which actually performs inference as a refresher metropolis hastings implements a very basic algorithm pick some initial state theta 0 for t in 1 n do a generate a proposal parameterization t sim q theta t mid theta t 1 b calculate the acceptance probability alpha text min big 1 frac pi t pi theta t 1 frac q t 1 mid t q t mid t 1 big c if u le where u sim 0 1 then theta t theta t otherwise theta t theta t 1 of course it s much easier to do this in the log space so the acceptance probability is more commonly written as alpha min big log pi t log pi t 1 log q t 1 mid t log q t mid t 1 0 big in interface terms we should do the following make a new transition containing a proposed sample calculate the acceptance probability if we accept return the new transition otherwise return the old one stepsthe step function is the function that performs the bulk of your inference in our case we will implement two step functions one for the very first iteration and one for every subsequent iteration define the first step function which is called at the beginning of sampling return the initial parameter used to define the sampler function abstractmcmc step rng abstractrng model densitymodel spl metropolishastings n integer nothing kwargs return transition model spl init endthe first step function just packages up the initial parameterization inside the sampler and returns it we implicity accept the very first parameterization the other step function performs the usual steps from metropolis hastings included are several helper functions proposal and q which are designed to replicate the functions in the pseudocode above proposal generates a new proposal in the form of a transition which can be univariate if the value passed in is univariate or it can be multivariate if the transition given is multivariate proposals use a basic normal or mvnormal proposal distribution q returns the log density of one parameterization conditional on another according to the proposal distribution step generates a new proposal checks the acceptance probability and then returns either the previous transition or the proposed transition define a function that makes a basic proposal depending on a univariate parameterization or a multivariate parameterization propose spl metropolishastings model densitymodel real transition model rand spl proposal propose spl metropolishastings model densitymodel vector lt real transition model rand spl proposal propose spl metropolishastings model densitymodel t transition propose spl model t calculates the probability q cond using the proposal distribution spl proposal q spl metropolishastings real cond real logpdf spl proposal cond q spl metropolishastings vector lt real cond vector lt real logpdf spl proposal cond q spl metropolishastings t1 transition t2 transition q spl t1 t2 calculate the density of the model given some parameterization model densitymodel model model densitymodel t transition t lp define the other step function returns a transition containing either a new proposal if accepted or the previous proposal if not accepted function abstractmcmc step rng abstractrng model densitymodel spl metropolishastings integer prev transition kwargs generate a new proposal propose spl model prev calculate the log acceptance probability model model prev q spl prev q spl prev decide whether to return the previous or the new one if log rand rng lt min 0 0 return else return prev endendchainsin the default implementation sample just returns a vector of all transitions if instead you would like to obtain a chains object e g to simplify downstream analysis you have to implement the bundle samples function as well it accepts the vector of transitions and returns a collection of samples fortunately our transition is incredibly simple and we only need to build a little bit of functionality to accept custom parameter names passed in by the user a basic chains constructor that works with the transition struct we defined function abstractmcmc bundle samples rng abstractrng densitymodel s metropolishastings n integer ts vector lt transition chain type type any param names missing kwargs turn all the transitions into a vector of vectors vals copy reduce hcat vcat t t lp for t in ts check if we received any parameter names if ismissing param names param names parameter i for i in 1 length first vals 1 end add the log density field to the parameter names push param names lp bundle everything up and return a chains struct return chains vals param names internals lp endall done you can even implement different output formats by implementing bundle samples for different chain types which can be provided as keyword argument to sample as default sample uses chain type any testing the implementationnow that we have all the pieces we should test the implementation by defining a model to calculate the mean and variance parameters of a normal distribution we can do this by constructing a target density function providing a sample of data and then running the sampler with sample generate a set of data from the posterior we want to estimate data rand normal 5 3 30 define the components of a basic model insupport 2 gt 0dist normal 1 2 density insupport sum logpdf dist data inf construct a densitymodel model densitymodel density set up our sampler with initial parameters spl metropolishastings 0 0 0 0 sample from the posterior chain sample model spl 100000 param names if all the interface functions have been extended properly you should get an output from display chain that looks something like this object of type chains with data of type 100000 3 1 array float64 3 iterations 1 100000thinning interval 1chains 1samples per chain 100000internals lpparameters 2 element array chaindataframe 1 summary statistics row parameters mean std naive se mcse ess r hat symbol float64 float64 float64 float64 any any 1 5 33157 0 854193 0 0027012 0 00893069 8344 75 1 00009 2 4 54992 0 632916 0 00200146 0 00534942 14260 8 1 00005 quantiles row parameters 2 5 25 0 50 0 75 0 97 5 symbol float64 float64 float64 float64 float64 1 3 6595 4 77754 5 33182 5 89509 6 99651 2 3 5097 4 09732 4 47805 4 93094 5 96821 it looks like we re extremely close to our true parameters of normal 5 3 though with a fairly high variance due to the low sample size conclusionwe ve seen how to implement the sampling interface for general projects turing s interface methods are ever evolving so please open an issue at abstractmcmc with feature requests or problems", "title": "Interface Guide"},{"location": "/docs/for-developers/variational_inference", "text": "overviewin this post we ll have a look at what s know as variational inference vi a family of approximate bayesian inference methods in particular we will focus on one of the more standard vi methods called automatic differentation variational inference advi ifhere we ll have a look at the theory behind vi but if you re interested in how to use advi in turing jl checkout this tutorial motivationin bayesian inference one usually specifies a model as follows given data x i i 1 n begin align text prior quad z amp sim p z text likelihood quad x i amp overset text i i d sim p x mid z quad text where quad i 1 dots n end align where overset text i i d sim denotes that the samples are identically independently distributed our goal in bayesian inference is then to find the posterior p z mid x i i 1 n prod i 1 n p z mid x i in general one cannot obtain a closed form expression for p z mid x i i 1 n but one might still be able to sample from p z mid x i i 1 n with guarantees of converging to the target posterior p z mid x i i 1 n as the number of samples go to infty e g mcmc as you are hopefully already aware turing jl provides a lot of different methods with asymptotic exactness guarantees that we can apply to such a problem unfortunately these unbiased samplers can be prohibitively expensive to run as the model p increases in complexity the convergence of these unbiased samplers can slow down dramatically still in the infinite limit these methods should converge to the true posterior but infinity is fairly large like at least more than 12 so this might take a while in such a case it might be desirable to sacrifice some of these asymptotic guarantees and instead approximate the posterior p z mid x i i 1 n using some other model which we ll denote q z there are multiple approaches to take in this case one of which is variational inference vi variational inference vi in vi we re looking to approximate p z mid x i i 1 n using some approximate or variational posterior q z to approximate something you need a notion of what close means in the context of probability densities a standard such measure of closeness is the kullback leibler kl divergence though this is far from the only one the kl divergence is defined between two densities q z and p z mid x i i 1 n as begin align mathrm d kl big q z p z mid x i i 1 n big amp int log bigg frac q z prod i 1 n p z mid x i bigg q z mathrm d z amp mathbb e z sim q z big log q z sum i 1 n log p z mid x i big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p z mid x i big end align it s worth noting that unfortunately the kl divergence is not a metric distance in the analysis sense due to its lack of symmetry on the other hand it turns out that minimizing the kl divergence that it s actually equivalent to maximizing the log likelihood also under reasonable restrictions on the densities at hand mathrm d kl big q z p z mid x i i 1 n big 0 quad iff quad p z p z mid x i i 1 n quad forall z therefore one could and we will attempt to approximate p z mid x i i 1 n using a density q z by minimizing the kl divergence between these two one can also show that mathrm d kl ge 0 which we ll need later finally notice that the kl divergence is only well defined when in fact q z is zero everywhere p z mid x i i 1 n is zero i e mathrm supp big q z big subseteq mathrm supp big p z mid x big otherwise there might be a point z 0 sim q z such that p z 0 mid x i i 1 n 0 resulting in log big frac q z 0 big which doesn t make sense one major problem as we can see in the definition of the kl divergence we need p z mid x i i 1 n for any z if we want to compute the kl divergence between this and q z we don t have that the entire reason we even do bayesian inference is that we don t know the posterior cleary this isn t going to work or is it computing kl divergence without knowing the posteriorfirst off recall that p z mid x i frac p x i z p x i so we can write begin align mathrm d kl big q z p z mid x i i 1 n big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p x i z log p x i big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p x i z big sum i 1 n mathbb e z sim q z big log p x i big amp mathbb e z sim q z big log q z big sum i 1 n mathbb e z sim q z big log p x i z big sum i 1 n log p x i end align where in the last equality we used the fact that p x i is independent of z now you re probably thinking oh great now you ve introduced p x i which we also can t compute in general woah calm down human let s do some more algebra the above expression can be rearranged to mathrm d kl big q z p z mid x i i 1 n big underbrace sum i 1 n mathbb e z sim q z big log p x i z big mathbb e z sim q z big log q z big mathrm elbo q underbrace sum i 1 n mathbb e z sim q z big log p x i big text constant see the left hand side is constant and as we mentioned before mathrm d kl ge 0 what happens if we try to maximize the term we just gave the completely arbitrary name mathrm elbo well if mathrm elbo goes up while p x i stays constant then mathrm d kl has to go down that is the q z which minimizes the kl divergence is the same q z which maximizes mathrm elbo q underset q mathrm argmin mathrm d kl big q z p z mid x i i 1 n big underset q mathrm argmax mathrm elbo q where begin align mathrm elbo q amp bigg sum i 1 n mathbb e z sim q z big log p x i z big bigg mathbb e z sim q z big log q z big amp bigg sum i 1 n mathbb e z sim q z big log p x i z big bigg mathbb h big q z big end align and mathbb h big q z big denotes the differential entropy of q z assuming joint p x i z and the entropy mathbb h big q z big are both tractable we can use a monte carlo for the remaining expectation this leaves us with the following tractable expression underset q mathrm argmin mathrm d kl big q z p z mid x i i 1 n big approx underset q mathrm argmax widehat mathrm elbo q where widehat mathrm elbo q frac 1 m bigg sum k 1 m sum i 1 n log p x i z k bigg mathbb h big q z big quad text where quad z k sim q z quad forall k 1 dots m hence as long as we can sample from q z somewhat efficiently we can indeed minimize the kl divergence neat eh sidenote in the case where q z is tractable but mathbb h big q z big is not we can use an monte carlo estimate for this term too but this generally results in a higher variance estimate also i fooled you real good the elbo isn t an arbitrary name hah in fact it s an abbreviation for the expected lower bound elbo because it uhmm well it s the expected lower bound remember mathrm d kl ge 0 yup maximizing the elbofinding the optimal q over all possible densities of course isn t feasible instead we consider a family of parameterized densities mathscr d theta where theta denotes the space of possible parameters each density in this family q theta in mathscr d theta is parameterized by a unique theta in theta moreover we ll assume q theta z i e evaluating the probability density q at any point z is differentiable z sim q theta z i e the process of sampling from q theta z is differentiable 1 is fairly straight forward but 2 is a bit tricky what does it even mean for a sampling process to be differentiable this is quite an interesting problem in its own right and would require something like a 50 page paper to properly review the different approaches highly recommended read we re going to make use of a particular such approach which goes under a bunch of different names reparametrization trick path derivative etc this refers to making the assumption that all elements q theta in mathscr q theta can be considered as reparameterizations of some base density say bar q z that is if q theta in mathscr q theta then z sim q theta z quad iff quad z g theta tilde z quad text where quad bar z sim bar q z for some function g theta differentiable wrt theta so all q theta in mathscr q theta are using the same reparameterization function g but each q theta correspond to different choices of theta for f theta under this assumption we can differentiate the sampling process by taking the derivative of g theta wrt theta and thus we can differentiate the entire widehat mathrm elbo q theta wrt theta with the gradient available we can either try to solve for optimality either by setting the gradient equal to zero or maximize widehat mathrm elbo q theta stepwise by traversing mathscr q theta in the direction of steepest ascent for the sake of generality we re going to go with the stepwise approach with all this nailed down we eventually reach the section on automatic differentiation variational inference advi automatic differentiation variational inference advi so let s revisit the assumptions we ve made at this point the variational posterior q theta is in a parameterized family of densities denoted mathscr q theta with theta in theta mathscr q theta is a space of reparameterizable densities with bar q z as the base density the parameterization function g theta is differentiable wrt theta evaluation of the probability density q theta z is differentiable wrt theta mathbb h big q theta z big is tractable evaluation of the joint density p x z is tractable and differentiable wrt z the support of p z mid x is a subspace of the support of q z mathrm supp big p z mid x big subseteq mathrm supp big q z big all of these are not necessary to do vi but they are very convenient and results in a fairly flexible approach one distribution which has a density satisfying all of the above assumptions except 7 we ll get back to this in second for any tractable and differentiable p z mid x i i 1 n is the good ole gaussian normal distribution z sim mathcal n mu sigma quad iff quad z g mu l bar z mu l t tilde z quad text where quad bar z sim bar q z mathcal n 1 d i d times d where sigma l l t with l obtained from the cholesky decomposition abusing notation a bit we re going to write theta mu sigma mu 1 dots mu d l 11 dots l 1 d l 2 1 dots l 2 d dots l d 1 dots l d d with this assumption we finally have a tractable expression for widehat mathrm elbo q mu sigma well assuming 7 is holds since a gaussian has non zero probability on the entirety of mathbb r d we also require p z mid x i i 1 n to have non zero probability on all of mathbb r d though not necessary we ll often make a mean field assumption for the variational posterior q z i e assume independence between the latent variables in this case we ll write theta mu sigma 2 mu 1 dots mu d sigma 1 2 dots sigma d 2 examplesas a trivial example we could apply the approach described above to is the following generative model for p z mid x i i 1 n begin align m amp sim mathcal n 0 1 x i amp overset text i i d mathcal n m 1 quad i 1 dots n end align in this case z m and we have the posterior defined p m mid x i i 1 n p m prod i 1 n p x i mid m then the variational posterior would be q mu sigma mathcal n mu sigma 2 quad text where quad mu in mathbb r sigma 2 in mathbb r and since prior of m mathcal n 0 1 has non zero probability on the entirety of mathbb r same as q m i e assumption 7 above holds everything is fine and life is good but what about this generative model for p z mid x i i 1 n begin align s amp sim mathrm inversegamma 2 3 m amp sim mathcal n 0 s x i amp overset text i i d mathcal n m s quad i 1 dots n end align with posterior p s m mid x i i 1 n p s p m mid s prod i 1 n p x i mid s m and the mean field variational posterior q s m will be q mu 1 mu 2 sigma 1 2 sigma 2 2 s m p mathcal n mu 1 sigma 1 2 s p mathcal n mu 2 sigma 2 2 m where we ve denoted the evaluation of the probability density of a gaussian as p mathcal n mu sigma 2 x observe that mathrm inversegamma 2 3 has non zero probability only on mathbb r 0 infty which is clearly not all of mathbb r like q s m has i e mathrm supp big q s m big not subseteq mathrm supp big p z mid x i i 1 n big recall from the definition of the kl divergence that when this is the case the kl divergence isn t well defined this gets us to the automatic part of advi automatic how for a lot of the standard continuous densities p we can actually construct a probability density tilde p with non zero probability on all of mathbb r by transforming the constrained probability density p to tilde p in fact in these cases this is a one to one relationship as we ll see this helps solve the support issue we ve been going on and on about transforming densities using change of variablesif we want to compute the probability of x taking a value in some set a subseteq mathrm supp big p x big we have to integrate p x over a i e mathbb p p x in a int a p x mathrm d x this means that if we have a differentiable bijection f mathrm supp big q x big to mathbb r d with differentiable inverse f 1 mathbb r d to mathrm supp big p x big we can perform a change of variables mathbb p p x in a int f 1 a p big f 1 y big big det mathcal j f 1 y big mathrm d y where mathcal j f 1 x denotes the jacobian of f 1 evaluted at x observe that this defines a probability distribution mathbb p tilde p big y in f 1 a big int f 1 a tilde p y mathrm d y since f 1 big mathrm supp p x big mathbb r d which has probability 1 this probability distribution has density tilde p y with mathrm supp big tilde p y big mathbb r d defined tilde p y p big f 1 y big big det mathcal j f 1 y big or equivalently tilde p big f x big frac p x big det mathcal j f x big due to the fact that big det mathcal j f 1 y big big det mathcal j f x big 1 note it s also necessary that the log abs det jacobian term is non vanishing this can for example be accomplished by assuming f to also be elementwise monotonic back to viso why is this is useful well we re looking to generalize our approach using a normal distribution to cases where the supports don t match up how about defining q z by begin align eta amp sim mathcal n mu sigma z amp f 1 eta end align where f 1 mathbb r d to mathrm supp big p z mid x big is a differentiable bijection with differentiable inverse then z sim q mu sigma z implies z in mathrm supp big p z mid x big as we wanted the resulting variational density is q mu sigma z p mathcal n mu sigma big f z big big det mathcal j f z big note that the way we ve constructed q z here is basically a reverse of the approach we described above here we sample from a distribution with support on mathbb r and transform to mathrm supp big p z mid x big if we want to write the elbo explicitly in terms of eta rather than z the first term in the elbo becomes begin align mathbb e z sim q mu sigma z big log p x i z big amp mathbb e eta sim mathcal n mu sigma bigg log frac p big x i f 1 eta big big det mathcal j f 1 eta big bigg amp mathbb e eta sim mathcal n mu sigma big log p big x i f 1 eta big big mathbb e eta sim mathcal n mu sigma big big det mathcal j f 1 eta big big end align the entropy is invariant under change of variables thus mathbb h big q mu sigma z big is simply the entropy of the normal distribution which is known analytically hence the resulting empirical estimate of the elbo is begin align widehat mathrm elbo q mu sigma amp frac 1 m bigg sum k 1 m sum i 1 n big log p big x i f 1 eta k big log big det mathcal j f 1 eta k big big bigg mathbb h big p mathcal n mu sigma z big amp text where quad z k sim mathcal n mu sigma quad forall k 1 dots m end align and maximizing this wrt mu and sigma is what s referred to as automatic differentation variational inference advi now if you want to try it out check out the tutorial on how to use advi in turing jl", "title": "Variational Inference"},{"location": "/docs/library/advancedhmc/", "text": "index advancedhmc abstractproposal advancedhmc abstracttrajectory advancedhmc abstracttrajectorysampler advancedhmc binarytree advancedhmc multinomialts advancedhmc multinomialts advancedhmc multinomialts advancedhmc nuts advancedhmc nuts advancedhmc nuts advancedhmc slicets advancedhmc slicets advancedhmc slicets advancedhmc termination advancedhmc termination advancedhmc termination advancedhmc transition advancedhmc a advancedhmc build tree advancedhmc combine advancedhmc find good stepsize advancedhmc isterminated advancedhmc isterminated advancedhmc maxabs advancedhmc mh accept ratio advancedhmc nom step size advancedhmc pm next advancedhmc simple pm next advancedhmc step size advancedhmc temper advancedhmc transitionfunctions advancedhmc find good stepsize method find a good initial leap frog step size via heuristic search statsbase sample method sample rng abstractrng h hamiltonian abstractproposal abstractvecormat t n samples int adaptor abstractadaptor noadaptation n adapts int min div n samples 10 1 000 drop warmup bool false verbose bool true progress bool false sample n samples samples using the proposal under hamiltonian h the randomness is controlled by rng if rng is not provided global rng will be used the initial point is given by the adaptor is set by adaptor for which the default is no adaptation it will perform n adapts steps of adaptation for which the default is the minimum of 1 000 and 10 of n samples drop warmup controls to drop the samples during adaptation phase or not verbose controls the verbosity progress controls whether to show the progress meter or not advancedhmc a method a single hamiltonian integration step note this function is intended to be used in find good stepsize only advancedhmc build tree method recursivly build a tree for a given depth j advancedhmc combine method combine treeleft binarytree treeright binarytree merge a left tree treeleft and a right tree treeright under given hamiltonian h then draw a new candidate sample and update related statistics for the resulting tree advancedhmc isterminated method isterminated h hamiltonian t binarytree lt classicnouturn detect u turn for two phase points zleft and zright under given hamiltonian h using the original no u turn cirterion ref https arxiv org abs 1111 4246 https arxiv org abs 1701 02434 advancedhmc isterminated method isterminated h hamiltonian t binarytree lt generalisednouturn detect u turn for two phase points zleft and zright under given hamiltonian h using the generalised no u turn criterion ref https arxiv org abs 1701 02434 advancedhmc maxabs method maxabs a b return the value with the largest absolute value advancedhmc mh accept ratio method perform mh acceptance based on energy i e negative log probability advancedhmc nom step size method nom step size abstractintegrator get the nominal integration step size the current integration step size may differ from this for example if the step size is jittered nominal step size is usually used in adaptation advancedhmc pm next method progress meter update with all trajectory stats iteration number and metric shown advancedhmc simple pm next method simple progress meter update without any show values advancedhmc step size function step size abstractintegrator get the current integration step size advancedhmc temper method temper lf temperedleapfrog r step namedtuple i is half lt tuple integer bool n steps int tempering step step is a named tuple with i being the current leapfrog iteration and is half indicating whether or not it s the first half momentum tempering step advancedhmc transition method transition abstracttrajectory i h hamiltonian z phasepoint make a mcmc transition from phase point z using the trajectory under hamiltonian h note this is a rng implicit fallback function for transition global rng h z types advancedhmc multinomialts type multinomialts f lt abstractfloat lt abstracttrajectorysamplermultinomial trajectory sampler carried during the building of the tree it contains the weight of the tree defined as the total probabilities of the leaves advancedhmc multinomialts method multinomialts s multinomialts h0 abstractfloat zcand phasepoint multinomial sampler for a trajectory consisting only a leaf node tree weight is the unnormalised energy of the leaf advancedhmc multinomialts method multinomial sampler for the starting single leaf tree log weights for leaf nodes are their unnormalised hamiltonian energies ref https github com stan dev stan blob develop src stan mcmc hmc nuts base nuts hpp l226 advancedhmc nuts type dynamic trajectory hmc using the no u turn termination criteria algorithm advancedhmc nuts method nuts args nuts multinomialts generalisednouturn args create an instance for the no u turn sampling algorithm with multinomial sampling and original no u turn criterion below is the doc for nuts s c nuts s c integrator i max depth int 10 max f 1000 0 where i lt abstractintegrator f lt abstractfloat s lt abstracttrajectorysampler c lt abstractterminationcriterion create an instance for the no u turn sampling algorithm advancedhmc nuts method nuts s c integrator i max depth int 10 max f 1000 0 where i lt abstractintegrator f lt abstractfloat s lt abstracttrajectorysampler c lt abstractterminationcriterion create an instance for the no u turn sampling algorithm advancedhmc slicets type slicets f lt abstractfloat lt abstracttrajectorysamplertrajectory slice sampler carried during the building of the tree it contains the slice variable and the number of acceptable condidates in the tree advancedhmc slicets method slice sampler for the starting single leaf tree slice variable is initialized advancedhmc slicets method slicets s slicets h0 abstractfloat zcand phasepoint create a slice sampler for a single leaf tree the slice variable is copied from the passed in sampler s and the number of acceptable candicates is computed by comparing the slice variable against the current energy advancedhmc abstractproposal type abstract markov chain monte carlo proposal advancedhmc abstracttrajectory type hamiltonian dynamics numerical simulation trajectories advancedhmc abstracttrajectorysampler type sampler carried during the building of the tree advancedhmc binarytree type a full binary tree trajectory with only necessary leaves and information stored advancedhmc termination type terminationtermination reasons dynamic due to stoping criteria numerical due to large energy deviation from starting possibly numerical errors advancedhmc termination method termination s multinomialts nt nuts h0 f h f where f lt abstractfloat check termination of a hamiltonian trajectory advancedhmc termination method termination s slicets nt nuts h0 f h f where f lt abstractfloat check termination of a hamiltonian trajectory advancedhmc transition type a transition that contains the phase point and other statistics of the transition", "title": "AdvancedHMC"},{"location": "/docs/library/", "text": "index turing binomiallogit turing flat turing flatpos turing orderedlogistic turing inference gibbs turing inference hmc turing inference hmcda turing inference is turing inference nuts turing inference pg turing inference smc libtask tarray libtask tzerosmodelling dynamicppl model macro model expr warn true macro to specify a probabilistic model if warn is true a warning is displayed if internal variable names are used in the model definition examplemodel definition model function model generator x default x y endto generate a model call model generator x value samplers dynamicppl sampler type sampler t generic interface for implementing inference algorithms an implementation of an algorithm should include the following a type specifying the algorithm and its parameters derived from inferencealgorithm a method of sample function that produces results of inference which is where actual inference happens dynamicppl translates models to chunks that call the modelling functions at specified points the dispatch is based on the value of a sampler variable to include a new inference algorithm implements the requirements mentioned above in a separate file then include that file at the end of this one turing inference gibbs type gibbs algs compositional mcmc interface gibbs sampling combines one or more sampling algorithms each of which samples from a different set of variables in a model example model gibbs example x begin v1 normal 0 1 v2 categorical 5 enduse pg for a v2 variable and use hmc for the v1 variable note that v2 is discrete so the pg sampler is more appropriatethan is hmc alg gibbs hmc 0 2 3 v1 pg 20 v2 tips hmc and nuts are fast samplers and can throw off particle basedmethods like particle gibbs you can increase the effectiveness of particle sampling by including more particles in the particle sampler source turing inference hmc type hmc float64 n leapfrog int hamiltonian monte carlo sampler with static trajectory arguments float64 the leapfrog step size to use n leapfrog int the number of leapfrop steps to use usage hmc 0 05 10 tips if you are receiving gradient errors when using hmc try reducing thestep size parameter e g original step sizesample gdemo 1 5 2 hmc 0 1 10 1000 reduced step size sample gdemo 1 5 2 hmc 0 01 10 1000 source turing inference hmcda type hmcda n adapts int float64 float64 float64 0 0 hamiltonian monte carlo sampler with dual averaging algorithm usage hmcda 200 0 65 0 3 arguments n adapts int numbers of samples to use for adaptation float64 target acceptance rate 65 is often recommended float64 target leapfrop length float64 0 0 inital step size 0 means automatically search by turing for more information please view the following paper arxiv link hoffman matthew d and andrew gelman the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo journal of machine learning research 15 no 1 2014 1593 1623 source turing inference is type is importance sampling algorithm note that this method is particle based and arrays of variables must be stored in a tarray object usage is example define a simple normal model with unknown mean and variance model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s x 1 normal m sqrt s x 2 normal m sqrt s return s mendsample gdemo 1 5 2 is 1000 source warning missing docstring missing docstring for mh check documenter s build log for details turing inference nuts type nuts n adapts int float64 max depth int 5 max float64 1000 0 float64 0 0 no u turn sampler nuts sampler usage nuts use default nuts configuration nuts 1000 0 65 use 1000 adaption steps and target accept ratio 0 65 arguments n adapts int the number of samples to use with adaptation float64 target acceptance rate for dual averaging max depth float64 maximum doubling tree depth max float64 maximum divergence during doubling tree float64 inital step size 0 means automatically searching using a heuristic procedure source turing inference pg type struct pg space r lt turing inference particleinferenceparticle gibbs sampler note that this method is particle based and arrays of variables must be stored in a tarray object fields nparticles int64 number of particles resampler any resampling algorithm source turing inference smc type struct smc space r lt turing inference particleinferencesequential monte carlo sampler fields resampler anysourcedistributions turing flat type flat lt continuousunivariatedistributiona distribution with support and density of one everywhere source turing flatpos type flatpos l real a distribution with a lower bound of l and a density of one at every x above l source turing binomiallogit type binomiallogit n lt real i lt integer a univariate binomial logit distribution source warning missing docstring missing docstring for vecbinomiallogit check documenter s build log for details turing orderedlogistic type orderedlogistic any cutpoints lt abstractvector an ordered logistic distribution sourcedata structures libtask tarray type tarray t dims implementation of data structures that automatically perform copy on write after task copying if current task is an existing key in s then return s current task otherwise returns current task s last task usage tarray dim example ta tarray 4 initfor i in 1 4 ta i i end assignarray ta convert to 4 element array int64 1 1 2 3 4 utilities libtask tzeros function tzeros dims construct a distributed array of zeros trailing arguments are the same as those accepted by tarray tzeros dim example tz tzeros 4 constructarray tz convert to 4 element array int64 1 0 0 0 0", "title": "API"},{"location": "/docs/library/bijectors/", "text": "index bijectors adbijector bijectors abstractbijector bijectors bijector bijectors composed bijectors inverse bijectors permute bijectors stacked bijectors bijector bijectors composel bijectors composer bijectors forward bijectors forward bijectors isclosedform bijectors logabsdetjac bijectors logabsdetjac bijectors logabsdetjacinv bijectors logabsdetjacinv bijectors logpdf with jac bijectors transformedfunctions bijectors bijector method bijector d distribution returns the constrained to unconstrained bijector for distribution d bijectors forward method forward b bijector x computes both transform and logabsdetjac in one forward pass and returns a named tuple rv b x logabsdetjac logabsdetjac b x this defaults to the call above but often one can re use computation in the computation of the forward pass and the computation of the logabsdetjac forward allows the user to take advantange of such efficiencies if they exist bijectors forward method forward d distribution forward d distribution num samples int returns a namedtuple with fields x y logabsdetjac and logpdf in the case where d isa transformeddistribution this means x rand d dist y d transform x logabsdetjac is the logabsdetjac of the forward transform logpdf is the logpdf of y not xin the case where d isa distribution this means x rand d y x logabsdetjac 0 0 logpdf is logpdf of x bijectors isclosedform method isclosedform b bijector boolisclosedform b inverse lt bijector boolreturns true or false depending on whether or not evaluation of b has a closed form implementation most bijectors have closed form evaluations but there are cases where this is not the case for example the inverse evaluation of planarlayer requires an iterative procedure to evaluate and thus is not differentiable bijectors logabsdetjac method computes the absolute determinant of the jacobian of the inverse transformation bijectors logabsdetjac method logabsdetjac b bijector x logabsdetjac ib inverse lt bijector y computes the log abs det j b x where j is the jacobian of the transform similarily for the inverse transform default implementation for inverse lt bijector is implemented as logabsdetjac of original bijector bijectors logabsdetjacinv method logabsdetjacinv b bijector y just an alias for logabsdetjac inv b y bijectors logabsdetjacinv method logabsdetjacinv td univariatetransformed y real logabsdetjacinv td multivariatetransformed y abstractvector lt real computes the logabsdetjac of the inverse transformation since rand td returns the transformed random variable bijectors logpdf with jac method logpdf with jac td univariatetransformed y real logpdf with jac td mvtransformed y abstractvector lt real logpdf with jac td matrixtransformed y abstractmatrix lt real makes use of the forward method to potentially re use computation and returns a tuple logpdf logabsdetjac bijectors transformed method transformed d distribution transformed d distribution b bijector couples distribution d with the bijector b by returning a transformeddistribution if no bijector is provided i e transformed d is called then transformed d bijector d is returned bijectors composel method composel ts bijector composed lt tuple constructs composed such that ts are applied left to right bijectors composer method composer ts bijector composed lt tuple constructs composed such that ts are applied right to left types bijectors adbijector type abstract type for a bijector n making use of auto differentation ad to implement jacobian and by impliciation logabsdetjac bijectors bijector type abstract type of bijectors with fixed dimensionality bijectors composed type composed ts a b1 bijector n b2 bijector n composed lt tuple composel ts bijector n composed lt tuple composer ts bijector n composed lt tuple where a refers to either tuple vararg lt bijector n a tuple of bijectors of dimensionality n abstractarray lt bijector n an array of bijectors of dimensionality na bijector representing composition of bijectors composel and composer results in a composed for which application occurs from left to right and right to left respectively note that all the alternative ways of constructing a composed returns a tuple of bijectors this ensures type stability of implementations of all relating methdos e g inv if you want to use an array as the container instead you can docomposed b1 b2 in general this is not advised since you lose type stability but there might be cases where this is desired e g if you have a insanely large number of bijectors to compose examplessimple examplelet s consider a simple example of exp julia gt using bijectors expjulia gt b exp exp 0 julia gt b bcomposed tuple exp 0 exp 0 0 exp 0 exp 0 julia gt b b 1 0 exp exp 1 0 evaluationtruejulia gt inv b b exp exp 1 0 1 0 inversiontruejulia gt logabsdetjac b b 1 0 determinant of jacobian3 718281828459045notesorderit s important to note that does what is expected mathematically which means that the bijectors are applied to the input right to left e g first applying b2 and then b1 b1 b2 x b1 b2 x gt truebut in the composed struct itself we store the bijectors left to right so thatcb1 b1 b2 gt composed ts b2 b1 cb2 composel b2 b1 gt composed ts b2 b1 cb1 x cb2 x b1 b2 x gt truestructure will result in flatten the composition structure while composel and composer preserve the compositional structure this is most easily seen by an example julia gt b exp exp 0 julia gt cb1 b b cb2 b b julia gt cb1 cb2 ts lt different exp 0 exp 0 exp 0 exp 0 julia gt cb1 cb2 ts isa ntuple 4 exp 0 truejulia gt bijectors composer cb1 cb2 ts composed tuple exp 0 exp 0 0 exp 0 exp 0 composed tuple exp 0 exp 0 0 exp 0 exp 0 julia gt bijectors composer cb1 cb2 ts isa tuple composed composed true bijectors inverse type inv b bijector inverse b bijector a bijector representing the inverse transform of b bijectors stacked type stacked bs stacked bs ranges stack bs bijector 0 where 0 means 0 dim bijector a bijector which stacks bijectors together which can then be applied to a vector where bs i bijector is applied to x ranges i unitrange int arguments bs can be either a tuple or an abstractarray of 0 and or 1 dimensional bijectors if bs is a tuple implementations are type stable using generated functions if bs is an abstractarray implementations are not type stable and use iterative methods ranges needs to be an iterable consisting of unitrange int length bs length ranges needs to be true examplesb1 logit 0 0 1 0 b2 identity 0 b stack b1 b2 b 0 0 1 0 b1 0 0 1 0 gt true bijectors abstractbijector type abstract type for a bijector bijectors permute type permute a lt bijector 1 a bijector implementation of a permutation the permutation is performed using a matrix of type a there are a couple of different ways to construct permute permute 0 1 1 0 will map 1 2 gt 2 1 permute 2 1 will map 1 2 gt 2 1 permute 2 2 gt 1 1 gt 2 will map 1 2 gt 2 1 permute 2 1 2 gt 2 1 will map 1 2 gt 2 1 if this is not clear the examples might be of help examplesa simple example is permuting a vector of size 3 julia gt b1 permute 0 1 0 1 0 0 0 0 1 permute array int64 2 0 1 0 1 0 0 0 0 1 julia gt b2 permute 2 1 3 specify all elements at oncepermute sparsearrays sparsematrixcsc float64 int64 2 1 1 0 1 2 1 0 3 3 1 0 julia gt b3 permute 3 2 gt 1 1 gt 2 element wisepermute sparsearrays sparsematrixcsc float64 int64 2 1 1 0 1 2 1 0 3 3 1 0 julia gt b4 permute 3 1 2 gt 2 1 block wisepermute sparsearrays sparsematrixcsc float64 int64 2 1 1 0 1 2 1 0 3 3 1 0 julia gt b1 a b2 a b3 a b4 atruejulia gt b1 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt b2 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt b3 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt b4 1 2 3 3 element array float64 1 2 0 1 0 3 0julia gt inv b1 permute linearalgebra transpose int64 array int64 2 0 1 0 1 0 0 0 0 1 julia gt inv b1 b1 1 2 3 3 element array float64 1 1 0 2 0 3 0", "title": "Bijectors"},{"location": "/docs/tutorials/index", "text": "tutorialsthis section contains tutorials on how to implement common models in turing if you prefer to have an interactive jupyter notebook please fork or download the turingtutorials repository a list of all the tutorials available can be found to the left the introduction tutorial contains an introduction to coin flipping with turing and a brief overview of probabalistic programming tutorials are under continuous development but there are some older version available at the turingtutorials within the old notebooks section some of these were built using prior versions of turing and may not function correctly but they can assist in the syntax used for common models if there is a tutorial you would like to request please open an issue on the turingtutorials repository", "title": "Tutorials"},{"location": "/docs/using-turing/advanced", "text": "advanced usagehow to define a customized distributionturing jl supports the use of distributions from the distributions jl package by extension it also supports the use of customized distributions by defining them as subtypes of distribution type of the distributions jl package as well as corresponding functions below shows a workflow of how to define a customized distribution using our own implementation of a simple uniform distribution as a simple example 1 define the distribution typefirst define a type of the distribution as a subtype of a corresponding distribution type in the distributions jl package struct customuniform lt continuousunivariatedistributionend2 implement sampling and evaluation of the log pdfsecond define rand and logpdf which will be used to run the model distributions rand rng abstractrng d customuniform rand rng sample in 0 1 distributions logpdf d customuniform x real zero x p x 1 logp x 03 define helper functionsin most cases it may be required to define some helper functions 3 1 domain transformationcertain samplers such as hmc require the domain of the priors to be unbounded therefore to use our customuniform as a prior in a model we also need to define how to transform samples from 0 1 to to do this we simply need to define the corresponding bijector from bijectors jl which is what turing jl uses internally to deal with constrained distributions to transform from 0 1 to we can use the logit bijector bijectors bijector d customuniform logit 0 1 you d do the exact same thing for continuousmultivariatedistribution and continuousmatrixdistribution for example wishart defines a distribution over positive definite matrices and so bijector returns a pdbijector when called with a wishart distribution as an argument for discrete distributions there is no need to define a bijector the identity bijector is used by default alternatively for univariatedistribution we can define the minimum and maximum of the distributiondistributions minimum d customuniform 0 distributions maximum d customuniform 1 and bijectors jl will return a default bijector called truncatedbijector which makes use of minimum and maximum derive the correct transformation internally turing basically does the following when it needs to convert a constrained distribution to an unconstrained distribution e g when sampling using hmc b bijector dist transformed dist transformed dist b results in distribution with transformed support correction for logpdfand then we can call rand and logpdf as usual where rand transformed dist returns a sample in the unconstrained space and logpdf transformed dist y returns the log density of the original distribution but with y living in the unconstrained space to read more about bijectors jl check out the project readme 3 2 vectorization supportthe vectorization syntax follows rv distribution which requires rand and logpdf to be called on multiple data points at once an appropriate implementation for flat is shown below distributions logpdf d flat x abstractvector lt real zero x model internalsthe model macro accepts a function definition and generates a turing model struct for use by the sampler models can be constructed by hand without the use of a macro taking the gdemo model as an example the two code sections below macro and macro free are equivalent using turing model gdemo x begin set priors s inversegamma 2 3 m normal 0 sqrt s observe each value of x x normal m sqrt s endsample gdemo 1 5 2 0 hmc 0 1 5 1000 using turing initialize a namedtuple containing our data variables data x 1 5 2 0 create the model function mf vi sampler ctx model begin set the accumulated logp to zero resetlogp vi x model args x assume s has an inversegamma distribution s lp turing inference tilde ctx sampler inversegamma 2 3 turing varname s vi add the lp to the accumulated logp acclogp vi lp assume m has a normal distribution m lp turing inference tilde ctx sampler normal 0 sqrt s turing varname m vi add the lp to the accumulated logp acclogp vi lp observe each value of x i according to a normal distribution lp turing inference dot tilde ctx sampler normal m sqrt s x vi acclogp vi lp end instantiate a model object model dynamicppl model mf data dyanamicppl modelgen nothing nothing sample the model chain sample model hmc 0 1 5 1000 task copyingturing copies julia tasks to deliver efficient inference algorithms but it also provides alternative slower implementation as a fallback task copying is enabled by default task copying requires we use the ctask facility which is provided by libtask to create tasks", "title": "Advanced Usage"},{"location": "/docs/using-turing/autodiff", "text": "automatic differentiationswitching ad modesturing supports four packages of automatic differentiation ad in the back end during sampling the default ad backend is forwarddiff for forward mode ad three reverse mode ad backends are also supported namely tracker zygote and reversediff zygote and reversediff are supported optionally if explicitly loaded by the user with using zygote or using reversediff next to using turing to switch between the different ad backends one can call function turing setadbackend backend sym where backend sym can be forwarddiff forwarddiff tracker tracker zygote zygote or reversediff reversediff jl when using reversediff to compile the tape only once and cache it for later use the user needs to load memoization jl first with using memoization then call turing setrdcache true however note that the use of caching in certain types of models can lead to incorrect results and or errors models for which the compiled tape can be safely cached are models with fixed size loops and no run time if statements compile time if statements are fine to empty the cache you can call turing emptyrdcache compositional sampling with differing ad modesturing supports intermixed automatic differentiation methods for different variable spaces the snippet below shows using forwarddiff to sample the mean m parameter and using the tracker based trackerad autodiff for the variance s parameter using turing define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end sample using gibbs and varying autodiff backends c sample gdemo 1 5 2 gibbs hmc turing forwarddiffad 1 0 1 5 m hmc turing trackerad 0 1 5 s 1000 generally trackerad is faster when sampling from variables of high dimensionality greater than 20 and forwarddiffad is more efficient for lower dimension variables this functionality allows those who are performance sensistive to fine tune their automatic differentiation for their specific models if the differentation method is not specified in this way turing will default to using whatever the global ad backend is currently this defaults to forwarddiff", "title": "Automatic Differentiation"},{"location": "/docs/using-turing/dynamichmc", "text": "using dynamichmcturing supports the use of dynamichmc as a sampler through the dynamicnuts function dynamicnuts is not appropriate for use in compositional inference if you intend to use gibbs sampling you must use turing s native nuts function to use the dynamicnuts function you must import the dynamichmc package as well as turing turing does not formally require dynamichmc but will include additional functionality if both packages are present here is a brief example of how to apply dynamicnuts import turing and dynamichmc using logdensityproblems dynamichmc turing model definition model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end pull 2 000 samples using dynamicnuts chn sample gdemo 1 5 2 0 dynamicnuts 2000", "title": "Using DynamicHMC"},{"location": "/docs/using-turing/get-started", "text": "getting startedinstallationto use turing you need to install julia first and then install turing install juliayou will need to install julia 1 0 or greater which you can get from the official julia website install turing jlturing is an officially registered julia package so the following will install a stable version of turing while inside julia s package manager press from the repl add turingif you want to use the latest version of turing with some experimental features you can try the following instead add turing mastertest turingif all tests pass you re ready to start using turing examplehere s a simple example showing the package in action using turingusing statsplots define a simple normal model with unknown mean and variance model gdemo x y begin s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s end run sampler collect resultschn sample gdemo 1 5 2 hmc 0 1 5 1000 summarise results currently requires the master branch from mcmcchains describe chn plot and save resultsp plot chn savefig gdemo plot png", "title": "Getting Started"},{"location": "/docs/using-turing/guide", "text": "guidebasicsintroductiona probabilistic program is julia code wrapped in a model macro it can use arbitrary julia code but to ensure correctness of inference it should not have external effects or modify global state stack allocated variables are safe but mutable heap allocated objects may lead to subtle bugs when using task copying to help avoid those we provide a turing safe datatype tarray that can be used to create mutable arrays in turing programs to specify distributions of random variables turing programs should use the notation x distr where x is a symbol and distr is a distribution if x is undefined in the model function inside the probabilistic program this puts a random variable named x distributed according to distr in the current scope distr can be a value of any type that implements rand distr which samples a value from the distribution distr if x is defined this is used for conditioning in a style similar to anglican another ppl in this case x is an observed value assumed to have been drawn from the distribution distr the likelihood is computed using logpdf distr y the observe statements should be arranged so that every possible run traverses all of them in exactly the same order this is equivalent to demanding that they are not placed inside stochastic control flow available inference methods include importance sampling is sequential monte carlo smc particle gibbs pg hamiltonian monte carlo hmc hamiltonian monte carlo with dual averaging hmcda and the no u turn sampler nuts simple gaussian demobelow is a simple gaussian demo illustrate the basic usage of turing jl import packages using turingusing statsplots define a simple normal model with unknown mean and variance model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endnote as a sanity check the expectation of s is 49 24 2 04166666 and the expectation of m is 7 6 1 16666666 we can perform inference by using the sample function the first argument of which is our probabalistic program and the second of which is a sampler more information on each sampler is located in the api run sampler collect results c1 sample gdemo 1 5 2 smc 1000 c2 sample gdemo 1 5 2 pg 10 1000 c3 sample gdemo 1 5 2 hmc 0 1 5 1000 c4 sample gdemo 1 5 2 gibbs pg 10 m hmc 0 1 5 s 1000 c5 sample gdemo 1 5 2 hmcda 0 15 0 65 1000 c6 sample gdemo 1 5 2 nuts 0 65 1000 the mcmcchains module which is re exported by turing provides plotting tools for the chain objects returned by a sample function see the mcmcchains repository for more information on the suite of tools available for diagnosing mcmc chains summarise resultsdescribe c3 plot resultsplot c3 savefig gdemo plot png the arguments for each sampler are smc number of particles pg number of particles number of iterations hmc leapfrog step size leapfrog step numbers gibbs component sampler 1 component sampler 2 hmcda total leapfrog length target accept ratio nuts number of adaptation steps optional target accept ratio for detailed information on the samplers please review turing jl s api documentation modelling syntax explainedusing this syntax a probabilistic model is defined in turing the model function generated by turing can then be used to condition the model onto data subsequently the sample function can be used to generate samples from the posterior distribution in the following example the defined model is conditioned to the date arg1 1 arg2 2 by passing 1 2 to the model function model function model name arg 1 arg 2 endthe conditioned model can then be passed onto the sample function to run posterior inference model func model name 1 2 chn sample model func hmc perform inference by sampling using hmc the returned chain contains samples of the variables in the model var 1 mean chn var 1 taking the mean of a variable named var 1 the key var 1 can be a symbol or a string for example to fetch x 1 one can use chn symbol x 1 or chn x 1 the benefit of using a symbol to index allows you to retrieve all the parameters associated with that symbol as an example if you have the parameters x 1 x 2 and x 3 calling chn x will return a new chain with only x 1 x 2 and x 3 turing does not have a declarative form more generally the order in which you place the lines of a model macro matters for example the following example works define a simple normal model with unknown mean and variance model function model function y s poisson 1 y normal s 1 return yendsample model function 10 smc 100 but if we switch the s poisson 1 and y normal s 1 lines the model will no longer sample correctly define a simple normal model with unknown mean and variance model function model function y y normal s 1 s poisson 1 return yendsample model function 10 smc 100 sampling multiple chainsturing supports distributed and threaded parallel sampling to do so call sample model sampler parallel type n n chains where parallel type can be either mcmcthreads or mcmcdistributed for thread and parallel sampling respectively having multiple chains in the same object is valuable for evaluating convergence some diagnostic functions like gelmandiag require multiple chains if you do not want parallelism or are on an older version julia you can sample multiple chains with the mapreduce function replace num chains below with however many chains you wish to sample chains mapreduce c gt sample model fun sampler 1000 chainscat 1 num chains the chains variable now contains a chains object which can be indexed by chain to pull out the first chain from the chains object use chains 1 the method is the same if you use either of the below parallel sampling methods multithreaded samplingif you wish to perform multithreaded sampling and are running julia 1 3 or greater you can call sample with the following signature using turing model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endendmodel gdemo 1 5 2 0 sample four chains using multiple threads each with 1000 samples sample model nuts mcmcthreads 1000 4 be aware that turing cannot add threads for you you must have started your julia instance with multiple threads to experience any kind of parallelism see the julia documentation for details on how to achieve this distributed samplingto perform distributed sampling using multiple processes you must first import distributed process parallel sampling can be done like so load distributed to add processes and the everywhere macro using distributed load turing using turing add four processes to use for sampling addprocs 4 initialize everything on all the processes note make sure to do this after you ve already loaded turing so each process does not have to precompile parallel sampling may fail silently if you do not do this everywhere using turing define a model on all processes everywhere model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend declare the model instance everywhere everywhere model gdemo 1 5 2 0 sample four chains using multiple processes each with 1000 samples sample model nuts mcmcdistributed 1000 4 sampling from an unconditional distribution the prior turing allows you to sample from a declared model s prior if you wish to draw a chain from the prior to inspect your prior distributions you can simply runchain sample model prior n samples you can also run your model as if it were a function from the prior distribution by calling the model without specifying inputs or a sampler in the below example we specify a gdemo model which returns two variables x and y the model includes x and y as arguments but calling the function without passing in x or y means that turing s compiler will assume they are missing values to draw from the relevant distribution the return statement is necessary to retrieve the sampled x and y values model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s return x yendassign the function with missing inputs to a variable and turing will produce a sample from the prior distribution samples from p x y g prior sample gdemo missing missing g prior sample output 0 685690547873451 1 1972706455914328 sampling from a conditional distribution the posterior treating observations as random variablesinputs to the model that have a value missing are treated as parameters aka random variables to be estimated sampled this can be useful if you want to simulate draws for that parameter or if you are sampling from a conditional distribution turing supports the following syntax model function gdemo x type t float64 where t if x missing initialize x if missing x vector t undef 2 end s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend construct a model with x missingmodel gdemo missing c sample model hmc 0 01 5 500 note the need to initialize x when missing since we are iterating over its elements later in the model the generated values for x can be extracted from the chains object using c x turing also supports mixed missing and non missing values in x where the missing ones will be treated as random variables to be sampled while the others get treated as observations for example model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endend x 1 is a parameter but x 2 is an observationmodel gdemo missing 2 4 c sample model hmc 0 01 5 500 default valuesarguments to turing models can have default values much like how default values work in normal julia functions for instance the following will assign missing to x and treat it as a random variable if the default value is not missing x will be assigned that value and will be treated as an observation instead using turing model function generative x missing type t float64 where t lt real if x missing initialize x when missing x vector t undef 10 end s inversegamma 2 3 m normal 0 sqrt s for i in 1 length x x i normal m sqrt s end return s mendm generative chain sample m hmc 0 01 5 1000 access values inside chainyou can access the values inside a chain several ways turn them into a dataframe object use their raw axisarray form create a three dimensional array objectfor example let c be a chain 1 dataframe c converts c to a dataframe 2 c value retrieves the values inside c as an axisarray and 3 c value data retrieves the values inside c as a 3d array variable types and type parametersthe element type of a vector or matrix of random variables should match the eltype of the its prior distribution lt integer for discrete distributions and lt abstractfloat for continuous distributions moreover if the continuous random variable is to be sampled using a hamiltonian sampler the vector s element type needs to either be 1 real to enable auto differentiation through the model which uses special number types that are sub types of real or 2 some type parameter t defined in the model header using the type parameter syntax e g gdemo x type t float64 where t begin similarly when using a particle sampler the julia variable used should either be 1 a tarray or 2 an instance of some type parameter t defined in the model header using the type parameter syntax e g gdemo x type t vector float64 where t begin querying probabilities from model or chainconsider the following gdemo model model function gdemo x y s inversegamma 2 3 m normal 0 sqrt s x normal m sqrt s y normal m sqrt s endthe following are examples of valid queries of the turing model or chain prob x 1 0 y 1 0 model gdemo s 1 0 m 1 0 calculates the likelihood of x 1 and y 1 given s 1 and m 1 prob s 1 0 m 1 0 model gdemo x nothing y nothing calculates the joint probability of s 1 and m 1 ignoring x and y x and y are ignored so they can be optionally dropped from the rhs of but it is recommended to define them prob s 1 0 m 1 0 x 1 0 model gdemo y nothing calculates the joint probability of s 1 m 1 and x 1 ignoring y prob s 1 0 m 1 0 x 1 0 y 1 0 model gdemo calculates the joint probability of all the variables after the mcmc sampling given a chain prob x 1 0 y 1 0 chain chain model gdemo calculates the element wise likelihood of x 1 0 and y 1 0 for each sample in chain if save state true was used during sampling i e sample model sampler n save state true you can simply do prob x 1 0 y 1 0 chain chain in all the above cases logprob can be used instead of prob to calculate the log probabilities instead maximum likelihood and maximum a posterior estimatesturing provides support for two mode estimation techniques maximum likelihood estimation mle and maximum a posterior map estimation optimization is performed by the optim jl package mode estimation is currently a optional tool and will not be available to you unless you have manually installed optim and loaded the package with a using statement to install optim run import pkg pkg add optim mode estimation only works when all model parameters are continuous discrete parameters cannot be estimated with mle map as of yet to understand how mode estimation works let us first load turing and optim to enable mode estimation and then declare a model note that loading optim explicitly is required for mode estimation to function as turing does not load the opimization suite unless optim is loaded as well using turingusing optim model function gdemo x s inversegamma 2 3 m normal 0 sqrt s for i in eachindex x x i normal m sqrt s endendonce the model is defined we can construct a model instance as we normally would create some data to pass to the model data 1 5 2 0 instantiate the gdemo model with our data model gdemo data mode estimation is typically quick and easy at this point turing extends the function optim optimize and accepts the structs mle or map which inform turing whether to provide an mle or map estimate respectively by default the lbfgs optimizer is used though this can be changed basic usage is generate a mle estimate mle estimate optimize model mle generate a map estimate map estimate optimize model map if you wish to change to a different optimizer such as neldermead simply place your optimizer in the third argument slot use neldermeadmle estimate optimize model mle neldermead use simulatedannealingmle estimate optimize model mle simulatedannealing use particleswarmmle estimate optimize model mle particleswarm use newtonmle estimate optimize model mle newton use acceleratedgradientdescentmle estimate optimize model mle acceleratedgradientdescent some methods may have trouble calculating the mode because not enough iterations were allowed or the target function moved upwards between function calls turing will warn you if optim fails to converge by running optim converge a typical solution to this might be to add more iterations or allow the optimizer to increase between function iterations increase the iterations and allow function eval to increase between calls mle estimate optimize model mle newton optim options iterations 10 000 allow f increases true more options for optim are available here analyzing your mode estimateturing extends several methods from statsbase that can be used to analyze your mode estimation results methods implemented include vcov informationmatrix coeftable params and coef among others for example let s examine our ml estimate from above using coeftable import statsbase to use it s statistical methods using statsbase print out the coefficient table coeftable mle estimate estimate stderror tstat s 0 0625 0 0625 1 0m 1 75 0 176777 9 8995 standard errors are calculated from the fisher information matrix inverse hessian of the log likelihood or log joint t statistics will be familiar to frequentist statisticians warning standard errors calculated in this way may not always be appropriate for map estimates so please be cautious in interpereting them sampling with the map mle as initial statesyou can begin sampling your chain from an mle map estimate by extracting the vector of parameter values and providing it to the sample function with the keyword init theta for example here is how to sample from the full posterior using the map estimate as the starting point generate an map estimate map estimate optimize model map sample with the map estimate as the starting point chain sample model nuts 1 000 init theta map estimate values array beyond the basicscompositional sampling using gibbsturing jl provides a gibbs interface to combine different samplers for example one can combine an hmc sampler with a pg sampler to run inference for different parameters in a single model as below model function simple choice xs p beta 2 2 z bernoulli p for i in 1 length xs if z 1 xs i normal 0 1 else xs i normal 2 1 end endendsimple choice f simple choice 1 5 2 0 0 3 chn sample simple choice f gibbs hmc 0 2 3 p pg 20 z 1000 the gibbs sampler can be used to specify unique automatic differentation backends for different variable spaces please see the automatic differentiation article for more for more details of compositional sampling in turing jl please check the corresponding paper working with mcmcchains jlturing jl wraps its samples using mcmcchains chain so that all the functions working for mcmcchains chain can be re used in turing jl two typical functions are mcmcchains describe and mcmcchains plot which can be used as follows for an obtained chain chn for more information on mcmcchains please see the github repository describe chn lists statistics of the samples plot chn plots statistics of the samples there are numerous functions in addition to describe and plot in the mcmcchains package such as those used in convergence diagnostics for more information on the package please see the github repository working with libtask jlthe libtask jl library provides write on copy data structures that are safe for use in turing s particle based samplers one data structure in particular is often required for use the tarray the following sampler types require the use of a tarray to store distributions ipmcmc is pg pmmh smcif you do not use a tarray to store arrays of distributions when using a particle based sampler you may experience errors here is an example of how the tarray using a tarray constructor function called tzeros can be applied in this way turing model definition model function bayeshmm y declare a tarray with a length of n s tzeros int n m vector real undef k t vector vector real undef k for i 1 k t i dirichlet ones k k m i normal i 0 01 end draw from a distribution for each element in s s 1 categorical k for i 2 n s i categorical vec t s i 1 y i normal m s i 0 1 end return s m end changing default settingssome of turing jl s default settings can be changed for better usage ad chunk sizeforwarddiff turing s default ad backend uses forward mode chunk wise ad the chunk size can be manually set by setchunksize new chunk size alternatively use an auto tuning helper function auto tune chunk size mf function rep num 10 which will profile various chunk sizes here mf is the model function e g gdemo 1 5 2 and rep num is the number of repetitions during profiling ad backendturing supports four packages of automatic differentiation ad in the back end during sampling the default ad backend is forwarddiff for forward mode ad three reverse mode ad backends are also supported namely tracker zygote and reversediff zygote and reversediff are supported optionally if explicitly loaded by the user with using zygote or using reversediff next to using turing for more information on turing s automatic differentiation backend please see the automatic differentiation article progress loggingturing jl uses progresslogging jl to log the progress of sampling progress logging is enabled as default but might slow down inference it can be turned on or off by setting the keyword argument progress of sample to true or false respectively moreover you can enable or disable progress logging globally by calling turnprogress true or turnprogress false respectively turing uses heuristics to select an appropriate visualization backend if you use juno the progress is displayed with a progress bar in the atom window for jupyter notebooks the default backend is consoleprogressmonitor jl in all other cases progress logs are displayed with terminalloggers jl alternatively if you provide a custom visualization backend turing uses it instead of the default backend", "title": "Guide"},{"location": "/docs/using-turing/index", "text": "turing documentationwelcome to the documentation for turing introductionturing is a general purpose probabilistic programming language for robust efficient bayesian inference and decision making current features include general purpose probabilistic programming with an intuitive modelling interface robust efficient hamiltonian monte carlo hmc sampling for differentiable posterior distributions particle mcmc sampling for complex posterior distributions involving discrete variables and stochastic control flow and compositional inference via gibbs sampling that combines particle mcmc hmc and random walk mh rwmh", "title": "Turing Documentation"},{"location": "/docs/using-turing/performancetips", "text": "performance tipsthis section briefly summarises a few common techniques to ensure good performance when using turing we refer to julialang org for general techniques to ensure good performance of julia programs use multivariate distributionsit is generally preferable to use multivariate distributions if possible the following example model gmodel x begin m normal for i 1 length x x i normal m 0 2 endendcan be directly expressed more efficiently using a simple transformation model gmodel x begin m normal x mvnormal fill m length x 0 2 endchoose your ad backendturing currently provides support for two different automatic differentiation ad backends generally try to use forwarddiff for models with few parameters and reversediff tracker or zygote for models with large parameter vectors or linear algebra operations see automatic differentiation for details special care for tracker and zygotein case of tracker and zygote it is necessary to avoid loops for now this is mainly due to the reverse mode ad backends tracker and zygote which are inefficient for such cases reversediff does better but vectorized operations will still perform better avoiding loops can be done using filldist dist n and arraydist dists filldist dist n creates a multivariate distribution that is composed of n identical and independent copies of the univariate distribution dist if dist is univariate or it creates a matrix variate distribution composed of n identical and idependent copies of the multivariate distribution dist if dist is multivariate filldist dist n m can also be used to create a matrix variate distribution from a univariate distribution dist arraydist dists is similar to filldist but it takes an array of distributions dists as input writing a custom distribution with a custom adjoint is another option to avoid loops make your model type stablefor efficient gradient based inference e g using hmc nuts or advi it is important to ensure the model is type stable we refer to julialang org for a general discussion on type stability the following example model tmodel x y begin p n size x params vector real undef n for i 1 n params i truncated normal 0 inf end a x params y mvnormal a 1 0 endcan be transformed into the following type stable representation model tmodel x y type t vector float64 where t begin p n size x params t undef n for i 1 n params i truncated normal 0 inf end a x params y mvnormal a 1 0 endnote that you can use code warntype to find type instabilities in your model definition for example consider the following simple program model tmodel x begin p vector real undef 1 p 1 normal p p 1 x normal p 1 endwe can usemodel tmodel 1 0 varinfo turing varinfo model spl turing samplefromprior code warntype model f model varinfo spl turing defaultcontext to inspect the type instabilities in the model reuse computations in gibbs samplingoften when performing gibbs sampling one can save computational time by caching the output of expensive functions the cached values can then be reused in future gibbs sub iterations which do not change the inputs to this expensive function for example in the following model model demo x begin a gamma b normal c function1 a d function2 b x normal c d endalg gibbs mh a mh b sample demo zeros 10 alg 1000 when only updating a in a gibbs sub iteration keeping b the same the value of d doesn t change and when only updating b the value of c doesn t change however if function1 and function2 are expensive and are both run in every gibbs sub iteration a lot of time would be spent computing values that we already computed before such a problem can be overcome using memoization jl memoizing a function lets us store and reuse the output of the function for every input it is called with this has a slight time overhead but for expensive functions the savings will be far greater to use memoization jl simply define memoized versions of function1 and function2 as such using memoization memoize memoized function1 args function1 args memoize memoized function2 args function2 args then define the turing model using the new functions as such model demo x begin a gamma b normal c memoized function1 a d memoized function2 b x normal c d end", "title": "Performance Tips"},{"location": "/docs/using-turing/quick-start", "text": "probablistic programming in thirty secondsif you are already well versed in probabalistic programming and just want to take a quick look at how turing s syntax works or otherwise just want a model to start with we have provided a bayesian coin flipping model to play with this example can be run on however you have julia installed see getting started but you will need to install the packages turing distributions mcmcchains and statsplots if you have not done so already this is an excerpt from a more formal example introducing probabalistic programming which can be found in jupyter notebook form here or as part of the documentation website here import libraries using turing statsplots random set the true probability of heads in a coin p true 0 5 iterate from having seen 0 observations to 100 observations ns 0 100 draw data from a bernoulli distribution i e draw heads or tails random seed 12 data rand bernoulli p true last ns declare our turing model model coinflip y begin our prior belief about the probability of heads in a coin p beta 1 1 the number of observations n length y for n in 1 n heads or tails of a coin are drawn from a bernoulli distribution y n bernoulli p endend settings of the hamiltonian monte carlo hmc sampler iterations 1000 0 05 10 start sampling chain sample coinflip data hmc iterations plot a summary of the sampling process for the parameter p i e the probability of heads in a coin histogram chain p", "title": "Probablistic Programming in Thirty Seconds"},{"location": "/docs/using-turing/sampler-viz", "text": "sampler visualizationintroductionthe codefor each sampler we will use the same code to plot sampler paths the block below loads the relevant libraries and defines a function for plotting the sampler s trajectory across the posterior the turing model definition used here is not especially practical but it is designed in such a way as to produce visually interesting posterior surfaces to show how different samplers move along the distribution env gks encoding utf 8 allows the use of unicode characters in plots jlusing plotsusing statsplotsusing turingusing bijectorsusing randomusing dynamicppl getlogp settrans getval reconstruct vectorize setval set a seed random seed 0 define a strange model model gdemo x begin s inversegamma 2 3 m normal 0 sqrt s bumps sin m cos m m m 5 bumps for i in eachindex x x i normal m sqrt s end return s mend define our data points x 1 5 2 0 13 0 2 1 0 0 set up the model call sample from the prior model gdemo x vi turing varinfo model convert the variance parameter to the real line before sampling note we only have to do this here because we are being very hands on turing will handle all of this for you during normal sampling dist inversegamma 2 3 svn vi metadata s vns 1 mvn vi metadata m vns 1 setval vi vectorize dist bijectors link dist reconstruct dist getval vi svn svn settrans vi true svn evaluate surface at coordinates function evaluate m1 m2 spl turing samplefromprior vi svn m1 vi mvn m2 model vi spl getlogp vi endfunction plot sampler chain label extract values from chain val get chain s m lp ss link ref inversegamma 2 3 val s ms val m lps val lp how many surface points to sample granularity 100 range start stop points spread 0 5 start minimum ss spread std ss stop maximum ss spread std ss start minimum ms spread std ms stop maximum ms spread std ms rng collect range start stop stop length granularity rng collect range start stop stop length granularity make surface plot p surface rng rng evaluate camera 30 65 ticks nothing colorbar false color inferno title label line range 1 length ms scatter3d ss line range ms line range lps line range mc viridis marker z collect line range msw 0 legend false colorbar false alpha 0 5 xlabel ylabel zlabel log probability title label return pend samplersgibbsgibbs sampling tends to exhibit a jittery trajectory the example below combines hmc and pg sampling to traverse the posterior c sample model gibbs hmc 0 01 5 s pg 20 m 1000 plot sampler c hmchamiltonian monte carlo hmc sampling is a typical sampler to use as it tends to be fairly good at converging in a efficient manner it can often be tricky to set the correct parameters for this sampler however and the nuts sampler is often easier to run if you don t want to spend too much time fiddling with step size and and the number of steps to take note however that hmc does not explore the positive values very well likely due to the leapfrop and step size parameter settings c sample model hmc 0 01 10 1000 plot sampler c hmcdathe hmcda sampler is an implementation of the hamiltonian monte carlo with dual averaging algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader c sample model hmcda 200 0 65 0 3 1000 plot sampler c mhmetropolis hastings mh sampling is one of the earliest markov chain monte carlo methods mh sampling does not move a lot unlike many of the other samplers implemented in turing typically a much longer chain is required to converge to an appropriate parameter estimate the plot below only uses 1 000 iterations of metropolis hastings c sample model mh 1000 plot sampler c as you can see the mh sampler doesn t move parameter estimates very often nutsthe no u turn sampler nuts is an implementation of the algorithm found in the paper the no u turn sampler adaptively setting path lengths in hamiltonian monte carlo by hoffman and gelman 2011 the paper can be found on arxiv for the interested reader nuts tends to be very good at traversing complex posteriors quickly c sample model nuts 0 65 1000 plot sampler c the only parameter that needs to be set other than the number of iterations to run is the target acceptance rate in the hoffman and gelman paper they note that a target acceptance rate of 0 65 is typical here is a plot showing a very high acceptance rate note that it appears to stick to a mode and is not particularly good at exploring the posterior as compared to the 0 65 target acceptance ratio case c sample model nuts 0 95 1000 plot sampler c an exceptionally low acceptance rate will show very few moves on the posterior c sample model nuts 0 2 1000 plot sampler c pgthe particle gibbs pg sampler is an implementation of an algorithm from the paper particle markov chain monte carlo methods by andrieu doucet and holenstein 2010 the interested reader can learn more here the two parameters are the number of particles and the number of iterations the plot below shows the use of 20 particles c sample model pg 20 1000 plot sampler c next we plot using 50 particles c sample model pg 50 1000 plot sampler c", "title": "Sampler Visualization"}]}
